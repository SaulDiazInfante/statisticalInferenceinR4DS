[
  {
    "objectID": "_cleaning_data_in_R_01.html#section",
    "href": "_cleaning_data_in_R_01.html#section",
    "title": "1  Common Data Problems",
    "section": "1.2 ",
    "text": "1.2 \n\nInstructions 100 XP\n\n\nex_002.R\n\n# Glimpse at bike_share_rides\nglimpse(bike_share_rides)\n\n# Summary of user_birth_year\nsummary(bike_share_rides$user_birth_year)\n\n# Convert user_birth_year to factor: user_birth_year_fct\nbike_share_rides <- bike_share_rides %>%\n  mutate(user_birth_year_fct = as.factor(user_birth_year))\n\n# Assert user_birth_year_fct is a factor\nassert_is_factor(bike_share_rides$user_birth_year_fct)\n\n# Summary of user_birth_year_fct\nsummary(bike_share_rides$user_birth_year_fct)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-1",
    "href": "_cleaning_data_in_R_01.html#section-1",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-2",
    "href": "_cleaning_data_in_R_01.html#section-2",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-3",
    "href": "_cleaning_data_in_R_01.html#section-3",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-4",
    "href": "_cleaning_data_in_R_01.html#section-4",
    "title": "1  Common Data Problems",
    "section": "1.7 ",
    "text": "1.7 \n\nInstructions 100 XP\n\n\nex_007.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-5",
    "href": "_cleaning_data_in_R_01.html#section-5",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-6",
    "href": "_cleaning_data_in_R_01.html#section-6",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-7",
    "href": "_cleaning_data_in_R_01.html#section-7",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-8",
    "href": "_cleaning_data_in_R_01.html#section-8",
    "title": "1  Common Data Problems",
    "section": "1.9 ",
    "text": "1.9 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "intro.html#overcome-common-data-problems-like-removing-duplicates-in-r",
    "href": "intro.html#overcome-common-data-problems-like-removing-duplicates-in-r",
    "title": "Introduction",
    "section": "Overcome Common Data Problems Like Removing Duplicates in R",
    "text": "Overcome Common Data Problems Like Removing Duplicates in R\nIt’s commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions.\nIn this course, you’ll learn a variety of techniques to help you clean dirty data using R. You’ll start by converting data types, applying range constraints, and dealing with full and partial duplicates to avoid double-counting."
  },
  {
    "objectID": "intro.html#delve-into-advanced-data-challenges",
    "href": "intro.html#delve-into-advanced-data-challenges",
    "title": "Introduction",
    "section": "Delve into Advanced Data Challenges",
    "text": "Delve into Advanced Data Challenges\nOnce you’ve practiced working on common data issues, you’ll move on to more advanced challenges such as ensuring consistency in measurements and dealing with missing data. After every new concept, you’ll have the chance to complete a hands-on exercise to cement your knowledge and build your experience."
  },
  {
    "objectID": "intro.html#learn-to-use-record-linkage-during-data-cleaning",
    "href": "intro.html#learn-to-use-record-linkage-during-data-cleaning",
    "title": "Introduction",
    "section": "Learn to Use Record Linkage During Data Cleaning",
    "text": "Learn to Use Record Linkage During Data Cleaning\nRecord Linkage is used to merge datasets together when the values have issues such as typos or different spellings. You’ll explore this useful technique in the final chapter and practice the application by using it to join two restaurant review datasets together into a single dataset."
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#converting-data-types",
    "href": "_cleaning_data_in_R_01.html#converting-data-types",
    "title": "1  Common Data Problems",
    "section": "1.1 Converting data types",
    "text": "1.1 Converting data types\nThroughout this chapter, you’ll be working with San Francisco bike share ride data called bike_share_rides. It contains information on start and end stations of each trip, the trip duration, and some user information.\nBefore beginning to analyze any dataset, it’s important to take a look at the different types of columns you’ll be working with, which you can do using glimpse().\nIn this exercise, you’ll take a look at the data types contained in bike_share_rides and see how an incorrect data type can flaw your analysis.\ndplyr and assertive are loaded and bike_share_rides is available.\n\nInstructions 100 XP\n\nExamine the data types of the columns of bike_share_rides.\nGet a summary of the user_birth_year column of bike_share_rides.\n\n\n\nex_001.R\n\n# Glimpse at bike_share_rides\nglimpse(bike_share_rides)\n\n# Summary of user_birth_year\nsummary(bike_share_rides$user_birth_year)\n\n# Convert user_birth_year to factor: user_birth_year_fct\nbike_share_rides <- bike_share_rides %>%\n  mutate(user_birth_year_fct = as.factor(user_birth_year))\n\n# Assert user_birth_year_fct is a factor\nassert_is_factor(bike_share_rides$user_birth_year_fct)\n\n# Summary of user_birth_year_fct\nsummary(bike_share_rides$user_birth_year_fct)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#trimming-strings",
    "href": "_cleaning_data_in_R_01.html#trimming-strings",
    "title": "1  Common Data Problems",
    "section": "1.2 Trimming strings",
    "text": "1.2 Trimming strings\nIn the previous exercise, you were able to identify the correct data type and convert user_birth_year to the correct type, allowing you to extract counts that gave you a bit more insight into the dataset.\nAnother common dirty data problem is having extra bits like percent signs or periods in numbers, causing them to be read in as characters. In order to be able to crunch these numbers, the extra bits need to be removed and the numbers need to be converted from character to numeric. In this exercise, you’ll need to convert the duration column from character to numeric, but before this can happen, the word “minutes” needs to be removed from each value.\ndplyr, assertive, and stringr are loaded and bike_share_rides is available.\n\nInstructions 100 XP\n\nUse str_remove() to remove \"minutes\" from the duration column of bike_share_rides. Add this as a new column called duration_trimmed.\nConvert the duration_trimmed column to a numeric type and add this as a new column called duration_mins.\nGlimpse at bike_share_rides and assert that the duration_mins column is numeric.\nCalculate the mean of duration_mins.\n\n\n\nex_002.R\n\nbike_share_rides <- bike_share_rides %>%\n  # Remove 'minutes' from duration: duration_trimmed\n  mutate(duration_trimmed = str_remove(duration, \"minutes\"),\n  # Convert duration_trimmed to numeric: duration_mins\n         duration_mins = as.numeric(duration_trimmed))\n\n# Glimpse at bike_share_rides\nglimpse(bike_share_rides)\n\n# Assert duration_mins is numeric\nassert_is_numeric(bike_share_rides$duration_mins)\n\n# Calculate mean duration\nmean(bike_share_rides$duration_mins)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#ride-duration-constraints",
    "href": "_cleaning_data_in_R_01.html#ride-duration-constraints",
    "title": "1  Common Data Problems",
    "section": "1.3 Ride duration constraints",
    "text": "1.3 Ride duration constraints\nValues that are out of range can throw off an analysis, so it’s important to catch them early on. In this exercise, you’ll be examining the duration_min column more closely. Bikes are not allowed to be kept out for more than 24 hours, or 1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned.\nIn this exercise, you’ll replace erroneous data with the range limit ( 1440 minutes), however, you could just as easily replace these values with NAs.\nload dplyr, assertive, and ggplot2 andbike_share_rides.\n\nInstructions 100 XP\nCreate a three-bin histogram of the duration_min column of bike_share_rides using ggplot2 to identify if there is out-of-range data.\n\n\nex_003.R\n\n# Create breaks\nbreaks <- \n  c(\n    min(bike_share_rides$duration_min),\n    0,\n    1440,\n    max(bike_share_rides$duration_min)\n  )\n\n# Create a histogram of duration_min\nggplot(bike_share_rides, aes(duration_min)) +\n  geom_histogram(breaks = breaks)\n\n# duration_min_const: replace vals of duration_min > 1440 with 1440\nbike_share_rides <- bike_share_rides %>%\n  mutate(\n    duration_min_const = replace(\n      duration_min,\n      duration_min > 1440 ,\n      1440\n    )\n  )\n\n# Make sure all values of \n# duration_min_const are between 0 and 1440\nassert_all_are_in_closed_range(\n  bike_share_rides$duration_min_const,\n  lower = 0, upper = 1440\n)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#back-to-the-future",
    "href": "_cleaning_data_in_R_01.html#back-to-the-future",
    "title": "1  Common Data Problems",
    "section": "1.4 Back to the future",
    "text": "1.4 Back to the future\nSomething has gone wrong and it looks like you have data with dates from the future, which is way outside of the date range you expected to be working with. To fix this, you’ll need to remove any rides from the dataset that have a date in the future. Before you can do this, the date column needs to be converted from a character to a Date. Having these as Date objects will make it much easier to figure out which rides are from the future, since R makes it easy to check if one Date object is before (<) or after (>) another.\nload dplyr, assertive and bike_share_rides.\n\nInstructions 100 XP\n\nConvert the date column of bike_share_rides from character to the Date data type.\nAssert that all values in the date column happened sometime in the past and not in the future.\nFilter bike_share_rides to get only the rides from the past or today, and save this as bike_share_rides_past.\nAssert that the dates in bike_share_rides_past occurred only in the past.\n\n\n\nex_004.R\n\nlibrary(lubridate)\n# Convert date to Date type\nbike_share_rides <- bike_share_rides %>%\n  mutate(date = as.Date(date))\n\n# Make sure all dates are in the past\nassert_all_are_in_past(bike_share_rides$date)\n\n\n# Filter for rides that occurred before or on today's date\nbike_share_rides_past <- bike_share_rides %>%\n  filter(date <= today())\n\n# Make sure all dates from bike_share_rides_past are in the past\nassert_all_are_in_past(bike_share_rides_past$date)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#full-duplicates",
    "href": "_cleaning_data_in_R_01.html#full-duplicates",
    "title": "1  Common Data Problems",
    "section": "1.5 Full duplicates",
    "text": "1.5 Full duplicates\nYou’ve been notified that an update has been made to the bike sharing data pipeline to make it more efficient, but that duplicates are more likely to be generated as a result. To make sure that you can continue using the same scripts to run your weekly analyses about ride statistics, you’ll need to ensure that any duplicates in the dataset are removed first.\nWhen multiple rows of a data frame share the same values for all columns, they’re full duplicates of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Each ride, including its ride_id should be unique.\nbe sure thatdplyr is loaded and bike_share_rides is available.\n\nInstructions 100 XP\n\nGet the total number of full duplicates in bike_share_rides.\nRemove all full duplicates from bike_share_rides and save the new data frame as bike_share_rides_unique.\nGet the total number of full duplicates in the new bike_share_rides_unique data frame.\n\n\n\nex_005.R\n\n# Count the number of full duplicate\n\nsum(duplicated(bike_share_rides))\n\n# Remove duplicates\nbike_share_rides_unique <- distinct(bike_share_rides)\n\n# Count the full duplicates in bike_share_rides_unique\nsum(duplicated(bike_share_rides_unique))"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#full-duplicates-1",
    "href": "_cleaning_data_in_R_01.html#full-duplicates-1",
    "title": "1  Common Data Problems",
    "section": "1.6 Full duplicates",
    "text": "1.6 Full duplicates\nYou’ve been notified that an update has been made to the bike sharing data pipeline to make it more efficient, but that duplicates are more likely to be generated as a result. To make sure that you can continue using the same scripts to run your weekly analyses about ride statistics, you’ll need to ensure that any duplicates in the dataset are removed first.\nWhen multiple rows of a data frame share the same values for all columns, they’re full duplicates of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Each ride, including its ride_id should be unique.\nbe sure thatdplyr is loaded and bike_share_rides is available.\n\nInstructions 100 XP\n\nGet the total number of full duplicates in bike_share_rides.\nRemove all full duplicates from bike_share_rides and save the new data frame as bike_share_rides_unique.\nGet the total number of full duplicates in the new bike_share_rides_unique data frame.\n\n\n\nex_006.R\n\n# Count the number of full duplicate\nsum(duplicated(bike_share_rides))\n# Remove duplicates\nbike_share_rides_unique <- distinct(bike_share_rides)\n# Count the full duplicates in bike_share_rides_unique\nsum(duplicated(bike_share_rides_unique))"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#removing-partial-duplicates",
    "href": "_cleaning_data_in_R_01.html#removing-partial-duplicates",
    "title": "1  Common Data Problems",
    "section": "1.6 Removing partial duplicates",
    "text": "1.6 Removing partial duplicates\nNow that you’ve identified and removed the full duplicates, it’s time to check for partial duplicates. Partial duplicates are a bit tricker to deal with than full duplicates. In this exercise, you’ll first identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first.\ndplyr is loaded and bike_share_rides is available.\n\nInstructions 100 XP\n\nCount the number of occurrences of each ride_id.\nFilter for ride_ids that occur multiple times.\nRemove full and partial duplicates from bike_share_rides based on ride_id only, keeping all columns.\nStore this as bike_share_rides_unique.\n\n\n\nex_006.R\n\n# Find duplicated ride_ids\nbike_share_rides %>% \n  count(ride_id) %>% \n  filter(n > 1)\n\n# Remove full and partial duplicates\nbike_share_rides_unique <- bike_share_rides %>%\n  # Only based on ride_id instead of all cols\n  distinct(ride_id, .keep_all = TRUE)\n\n# Find duplicated ride_ids in bike_share_rides_unique\nbike_share_rides_unique %>%\n  # Count the number of occurrences of each ride_id\n  count(ride_id) %>%\n  # Filter for rows with a count > 1\n  filter(n>1)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#aggregating-partial-duplicates",
    "href": "_cleaning_data_in_R_01.html#aggregating-partial-duplicates",
    "title": "1  Common Data Problems",
    "section": "1.7 Aggregating partial duplicates",
    "text": "1.7 Aggregating partial duplicates\nAnother way of handling partial duplicates is to compute a summary statistic of the values that differ between partial duplicates, such as mean, median, maximum, or minimum. This can come in handy when you’re not sure how your data was collected and want an average, or if based on domain knowledge, you’d rather have too high of an estimate than too low of an estimate (or vice versa).\ndplyr is loaded and bike_share_rides is available.\n\n1.7.1 Instructions 100 XP\n\nGroup bike_share_rides by ride_id and date.\nAdd a column called duration_min_avg that contains the mean ride duration for the row’s ride_id and date.\nRemove duplicates based on ride_id and date, keeping all columns of the data frame.\nRemove the duration_min column.\n\n\n\nex_007.R\n\nbike_share_rides %>%\n  # Group by ride_id and date\n  group_by(ride_id, date) %>%\n  # Add duration_min_avg column\n  mutate(duration_min_avg = mean(duration_min)) %>%\n  # Remove duplicates based on ride_id and date, keep all cols\n  distinct(ride_id, date, .keep_all = TRUE) %>%\n  # Remove duration_min column\n  select(-duration_min)"
  },
  {
    "objectID": "_cleaning_data_in_R_02.html#section",
    "href": "_cleaning_data_in_R_02.html#section",
    "title": "2  Categorical and Text Data",
    "section": "2.1 ",
    "text": "2.1 \n\n2.1.1 Instructions 100 XP\n{.r filename= ex_}"
  },
  {
    "objectID": "_cleaning_data_in_R_02.html",
    "href": "_cleaning_data_in_R_02.html",
    "title": "2  Categorical and Text Data",
    "section": "",
    "text": "3 Count cleanliness\nsfo_survey %>% count(cleanliness)"
  },
  {
    "objectID": "_cleaning_data_in_R_02.html#not-a-member",
    "href": "_cleaning_data_in_R_02.html#not-a-member",
    "title": "2  Categorical and Text Data",
    "section": "2.1 Not a member",
    "text": "2.1 Not a member\nNow that you’ve practiced identifying membership constraint problems, it’s time to fix these problems in a new dataset. Throughout this chapter, you’ll be working with a dataset called sfo_survey, containing survey responses from passengers taking flights from San Francisco International Airport (SFO). Participants were asked questions about the airport’s cleanliness, wait times, safety, and their overall satisfaction.\nThere were a few issues during data collection that resulted in some inconsistencies in the dataset. In this exercise, you’ll be working with the dest_size column, which categorizes the size of the destination airport that the passengers were flying to. A data frame called dest_sizes is available that contains all the possible destination sizes. Your mission is to find rows with invalid dest_sizes and remove them from the data frame.\nLoad dplyr, sfo_survey and dest_sizes.\n\nInstructions 100 XP\n\nCount the number of occurrences of each dest_size in sfo_survey.\n\n\n\nex_008.R\n\n\n# Count the number of occurrences of dest_size\nsfo_survey %>%\n  count(dest_size)\n\n# Find bad dest_size rows\nsfo_survey %>% \n  # Join with dest_sizes data frame to get bad dest_size rows\n  anti_join(dest_sizes, by = \"dest_size\") %>%\n  # Select id, airline, destination, and dest_size cols\n  select(id, airline, destination, dest_size)\n\n# Remove bad dest_size rows\nsfo_survey %>% \n  # Join with dest_sizes\n  semi_join(dest_sizes, by = \"dest_size\") %>%\n  # Count the number of each dest_size\n  count(dest_size)"
  },
  {
    "objectID": "_cleaning_data_in_R_02.html#correcting-inconsistency",
    "href": "_cleaning_data_in_R_02.html#correcting-inconsistency",
    "title": "2  Categorical and Text Data",
    "section": "2.3 Correcting inconsistency",
    "text": "2.3 Correcting inconsistency\nNow that you’ve identified that dest_size has whitespace inconsistencies and cleanliness has capitalization inconsistencies, you’ll use the new tools at your disposal to fix the inconsistent values in sfo_survey instead of removing the data points entirely, which could add bias to your dataset if more than 5% of the data points need to be dropped.\nLoad dplyr, stringr and sfo_survey.\n\nInstructions 100 XP\n\nAdd a column to sfo_survey called dest_size_trimmed that contains the values in the dest_size column with all leading and trailing whitespace removed.\nAdd another column called cleanliness_lower that contains the values in the cleanliness column converted to all lowercase.\nCount the number of occurrences of each category in dest_size_trimmed.\nCount the number of occurrences of each category in cleanliness_lower.\n\n\n\nex_010.R\n\n# Add new columns to sfo_survey\nsfo_survey <- sfo_survey %>%\n  # dest_size_trimmed: dest_size without whitespace\n  mutate(dest_size_trimmed =  str_trim(dest_size),\n         # cleanliness_lower: cleanliness converted to lowercase\n         cleanliness_lower = str_to_lower(cleanliness))\n\n# Count values of dest_size_trimmed\nsfo_survey %>%\n  count(dest_size_trimmed)\n\n# Count values of cleanliness_lower\nsfo_survey %>%\n  count(cleanliness_lower)"
  },
  {
    "objectID": "_cleaning_data_in_R_02.html#detecting-inconsistency-text-data",
    "href": "_cleaning_data_in_R_02.html#detecting-inconsistency-text-data",
    "title": "2  Categorical and Text Data",
    "section": "2.5 Detecting inconsistency text data",
    "text": "2.5 Detecting inconsistency text data\nYou’ve recently received some news that the customer support team wants to ask the SFO survey participants some follow-up questions. However, the auto-dialer that the call center uses isn’t able to parse all of the phone numbers since they’re all in different formats. After some investigation, you found that some phone numbers are written with hyphens (-) and some are written with parentheses ((,)). In this exercise, you’ll figure out which phone numbers have these issues so that you know which ones need fixing.\nLoad dplyr, stringr and sfo_survey.\n\nInstructions 100 XP\n\nFilter for rows with phone numbers that contain “-”s.\nFilter for rows with phone numbers that contain “(”, or “)”. Remember to use fixed() when searching for parentheses.\n\n\n\nex_012.R\n\n\n# Filter for rows with \"-\" in the phone column\nsfo_survey %>%\n  filter(str_detect(phone, \"-\"))\n\n# Filter for rows with \"(\" or \")\" in the phone column\nsfo_survey %>%\n  filter(str_detect(phone, fixed(\"(\")) | str_detect(phone, fixed(\")\")))"
  },
  {
    "objectID": "_cleaning_data_in_R_02.html#invalid-phone-numbers",
    "href": "_cleaning_data_in_R_02.html#invalid-phone-numbers",
    "title": "2  Categorical and Text Data",
    "section": "2.7 Invalid phone numbers",
    "text": "2.7 Invalid phone numbers\nThe customer support team is grateful for your work so far, but during their first day of calling participants, they ran into some phone numbers that were invalid. In this exercise, you’ll remove any rows with invalid phone numbers so that these faulty numbers don’t keep slowing the team down.\nMake dplyr, stringr and sfo_survey available.\n\nInstructions 100 XP\n\nExamine the invalid phone numbers by filtering for numbers whose length is not equal to 12.\nRemove the rows with invalid numbers by filtering for numbers with a length of exactly 12\n\n\n\nex_014.R\n\n# Check out the invalid numbers\nsfo_survey %>%\n  filter(str_length(phone) != 12)\n\n# Remove rows with invalid numbers\nsfo_survey %>%\n   filter(str_length(phone) == 12)"
  },
  {
    "objectID": "_cleaning_data_in_R_02.html#identifiying-inconsistency",
    "href": "_cleaning_data_in_R_02.html#identifiying-inconsistency",
    "title": "2  Categorical and Text Data",
    "section": "2.2 Identifiying inconsistency",
    "text": "2.2 Identifiying inconsistency\nIn the video exercise, you learned about different kinds of inconsistencies that can occur within categories, making it look like a variable has more categories than it should.\nIn this exercise, you’ll continue working with the sfo_survey dataset. You’ll examine the dest_size column again as well as the cleanliness column and determine what kind of issues, if any, these two categorical variables face.\nLoad dplyr and sfo_survey .\n\nInstructions 100 XP\n\nCount the number of occurrences of each category of the dest_size variable of sfo_survey.\n\n\n\nex_009.R\n\n# Count dest_size\n# Count dest_size\nsfo_survey %>%\n  count(dest_size)\n\n# Count cleanliness\nsfo_survey %>%\n  count(cleanliness)"
  },
  {
    "objectID": "_cleaning_data_in_R_02.html#collapsing-categories",
    "href": "_cleaning_data_in_R_02.html#collapsing-categories",
    "title": "2  Categorical and Text Data",
    "section": "2.4 Collapsing categories",
    "text": "2.4 Collapsing categories\nOne of the tablets that participants filled out the sfo_survey on was not properly configured, allowing the response for dest_region to be free text instead of a dropdown menu. This resulted in some inconsistencies in the dest_region variable that you’ll need to correct in this exercise to ensure that the numbers you report to your boss are as accurate as possible.\nMake dplyr, forcats and sfo_survey available.\n\nInstructions 100 XP\n\nCount the categories of dest_region.\n\n\n\nex_011.R\n\n# Count categories of dest_region\nsfo_survey %>%\n  count(dest_region)\n\n# Categories to map to Europe\neurope_categories <- c(\"EU\",\"eur\", \"Europ\")\n\n# Add a new col dest_region_collapsed\nsfo_survey %>%\n  # Map all categories in europe_categories to Europe\n  mutate(\n    dest_region_collapsed = fct_collapse(dest_region, \n                                     Europe = europe_categories)) %>%\n  # Count categories of dest_region_collapsed\n  count(dest_region_collapsed)"
  },
  {
    "objectID": "_cleaning_data_in_R_02.html#replacing-and-removing",
    "href": "_cleaning_data_in_R_02.html#replacing-and-removing",
    "title": "2  Categorical and Text Data",
    "section": "2.6 Replacing and removing",
    "text": "2.6 Replacing and removing\nIn the last exercise, you saw that the phone column of sfo_survey is plagued with unnecessary parentheses and hyphens. The customer support team has requested that all phone numbers be in the format \"123 456 7890\". In this exercise, you’ll use your new stringr skills to fulfill this request.\nMake dplyr, stringr and sfo_survey available.\n\nInstructions 100 XP\n\nRemove opening and closing parentheses from the phone column. Store this as a variable called phone_no_parens. Remember to use fixed()!\nAdd a new column to sfo_survey called phone_no_parens that contains the contents of phone_no_parens.\n\n\n\nex_013.R\n\n# Remove parentheses from phone column\nphone_no_parens <- sfo_survey$phone %>%\n  # Remove \"(\"s\n  str_remove(fixed(\"(\")) %>%\n  # Remove \")\"s\n  str_remove(fixed(\")\"))\n\n  # Remove parentheses from phone column\nphone_no_parens <- sfo_survey$phone %>%\n  # Remove \"(\"s\n  str_remove_all(fixed(\"(\")) %>%\n  # Remove \")\"s\n  str_remove_all(fixed(\")\"))\n\n# Add phone_no_parens as column\nsfo_survey %>%\n  mutate(phone_no_parens=phone_no_parens)\n\n# Remove parentheses from phone column\nphone_no_parens <- sfo_survey$phone %>%\n  # Remove \"(\"s\n  str_remove_all(fixed(\"(\")) %>%\n  # Remove \")\"s\n  str_remove_all(fixed(\")\"))\n\n# Add phone_no_parens as column\nsfo_survey %>%\n  mutate(\n    phone_no_parens = phone_no_parens,\n  # Replace all hyphens in phone_no_parens with spaces\n    phone_clean = str_replace_all(\n      phone_no_parens, \"-\", \" \"))"
  },
  {
    "objectID": "_cleaning_data_in_R_03.html#date-uniformity",
    "href": "_cleaning_data_in_R_03.html#date-uniformity",
    "title": "3  Advanced Data Problems",
    "section": "3.1 Date uniformity",
    "text": "3.1 Date uniformity\nIn this chapter, you work at an asset management company and you’ll be working with the accounts dataset, which contains information about each customer, the amount in their account, and the date their account was opened. Your boss has asked you to calculate some summary statistics about the average value of each account and whether the age of the account is associated with a higher or lower account value. Before you can do this, you need to make sure that the accounts dataset you’ve been given doesn’t contain any uniformity problems. In this exercise, you’ll investigate the date_opened column and clean it up so that all the dates are in the same format.\nMake dplyr and lubridate and accounts available. ### Instructions 100 XP {.unnumbered} Take a look at the head of accounts to get a sense of the data you’re working with.\n\n\nex_015.R\n\nhead(accounts)\n\n# Define the date formats\nformats <- c(\"%Y-%m-%d\", \"%B %d, %Y\")\n\n# Convert dates to the same format\naccounts %>%\n  mutate(date_opened_clean = parse_date_time(date_opened, formats))"
  },
  {
    "objectID": "_cleaning_data_in_R_03.html#currency-uniformity",
    "href": "_cleaning_data_in_R_03.html#currency-uniformity",
    "title": "3  Advanced Data Problems",
    "section": "3.2 Currency uniformity",
    "text": "3.2 Currency uniformity\nNow that your dates are in order, you’ll need to correct any unit differences. When you first plot the data, you’ll notice that there’s a group of very high values, and a group of relatively lower values. The bank has two different offices - one in New York, and one in Tokyo, so you suspect that the accounts managed by the Tokyo office are in Japanese yen instead of U.S. dollars. Luckily, you have a data frame called account_offices that indicates which office manages each customer’s account, so you can use this information to figure out which totals need to be converted from yen to dollars.\nThe formula to convert yen to dollars is USD = JPY / 104.\nload dplyr, ggplot2 , the accounts and account_offices data frames.\n\nInstructions 100 XP\n\nCreate a scatter plot with date_opened on the x-axis and total on the y-axis.\nLeft join accounts and account_offices by their id columns.\n\n\n\nex_016.R\n\n# Scatter plot of opening date and total amount\naccounts %>%\n  ggplot(aes(x = date_opened, y = total)) +\n  geom_point()\n\n# Left join accounts to account_offices by id\naccounts %>%\n  left_join(account_offices, by = \"id\") %>%\n  # Convert totals from the Tokyo office to USD\n  mutate(total_usd = ifelse(office == \"Tokyo\", total / 104, total)) %>%\n  # Scatter plot of opening date vs total_usd\n  ggplot(aes(x = date_opened, y = total_usd)) +\n    geom_point()"
  },
  {
    "objectID": "_cleaning_data_in_R_03.html#validating-totals",
    "href": "_cleaning_data_in_R_03.html#validating-totals",
    "title": "3  Advanced Data Problems",
    "section": "3.3 Validating totals",
    "text": "3.3 Validating totals\nIn this lesson, you’ll continue to work with the accounts data frame, but this time, you have a bit more information about each account. There are three different funds that account holders can store their money in. In this exercise, you’ll validate whether the total amount in each account is equal to the sum of the amount in fund_A, fund_B, and fund_C. If there are any accounts that don’t match up, you can look into them further to see what went wrong in the bookkeeping that led to inconsistencies.\nLoad dplyr and accounts.\n\nInstructions 100 XP\n\nCreate a new column called theoretical_total that contains the sum of the amounts in each fund.\nFind the accounts where the total doesn’t match the theoretical_total.\n\n\n\nex_017.R\n\n# Find invalid totals\naccounts %>%\n  # theoretical_total: sum of the three funds\n  mutate(\n    theoretical_total = fund_A + fund_B + fund_C\n  )%>%\n  # Find accounts where total doesn't match theoretical_total\n  filter(theoretical_total != total)"
  },
  {
    "objectID": "_cleaning_data_in_R_03.html#validating-age",
    "href": "_cleaning_data_in_R_03.html#validating-age",
    "title": "3  Advanced Data Problems",
    "section": "3.4 Validating age",
    "text": "3.4 Validating age\nNow that you found some inconsistencies in the total amounts, you’re suspicious that there may also be inconsistencies in the acct_age column, and you want tosee if these inconsistencies are related. Using the skills you learned from the video exercise, you’ll need to validate the age of each account and see if rows with inconsistent acct_ages are the same ones that had inconsistent totals\nMake dplyr and lubridate and accounts available.\n\nInstructions 100 XP\n\nCreate a new column called theoretical_age that contains the age of each account based on the date_opened.\nFind the accounts where the acct_age doesn’t match the theoretical_age.\n\n\n\nex_018.R\n\n# Find invalid acct_age\naccounts %>%\n  # theoretical_age: age of acct based on date_opened\n  mutate(theoretical_age = \n    floor(\n      as.numeric(\n        as.Date(date_opened) %--% today(),\n        \"years\"\n      )\n    )\n  ) %>%\n  # Filter for rows where acct_age is different from theoretical_age\n  filter(theoretical_age != acct_age)"
  },
  {
    "objectID": "_cleaning_data_in_R_03.html#visualizing-missing-data",
    "href": "_cleaning_data_in_R_03.html#visualizing-missing-data",
    "title": "3  Advanced Data Problems",
    "section": "3.5 Visualizing missing data",
    "text": "3.5 Visualizing missing data\nDealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data.\nYou just received a new version of the accounts data frame containing data on the amount held and amount invested for new and existing customers. However, there are rows with missing inv_amount values.\nYou know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness.\nLoad dplyr and visdat packages and accounts dataframe.\n\nInstructions 100 XP\n\nVisualize the missing values in accounts by column using a function from the visdat package.\nAdd a logical column to accounts called missing_inv that indicates whether each row is missing the inv_amount or not.\nGroup by missing_inv.\nCalculate the mean age for each group of missing_inv.\n\n\n\nex_019.R\n\nvis_miss(accounts)\n# Visualize the missing values by column\nvis_miss(accounts)\n\naccounts %>%\n  # missing_inv: Is inv_amount missing?\n  mutate(missing_inv = is.na(inv_amount)) %>%\n  # Group by missing_inv\n  group_by(missing_inv) %>%\n  # Calculate mean age for each missing_inv group\n  summarize(avg_age = mean(age))\n\n# Visualize the missing values by column\nvis_miss(accounts)\n\naccounts %>%\n  # missing_inv: Is inv_amount missing?\n  mutate(missing_inv = is.na(inv_amount)) %>%\n  # Group by missing_inv\n  group_by(missing_inv) %>%\n  # Calculate mean age for each missing_inv group\n  summarize(avg_age = mean(age))\n\n# Sort by age and visualize missing vals\naccounts %>%\n  arrange(age) %>%\n  vis_miss()"
  },
  {
    "objectID": "_cleaning_data_in_R_03.html#treating-missing-data",
    "href": "_cleaning_data_in_R_03.html#treating-missing-data",
    "title": "3  Advanced Data Problems",
    "section": "3.6 Treating missing data",
    "text": "3.6 Treating missing data\nIn this exercise, you’re working with another version of the accounts data that contains missing values for both the cust_id and acct_amount columns.\nYou want to figure out how many unique customers the bank has, as well as the average amount held by customers. You know that rows with missing cust_id don’t really help you, and that on average, the acct_amount is usually 5 times the amount of inv_amount.\nIn this exercise, you will drop rows of accounts with missing cust_ids, and impute missing values of inv_amount with some domain knowledge.\nLoad dplyr, assertive and accounts.\n\nInstructions 100 XP\n\nFilter accounts to remove rows with missing cust_ids and save as accounts_clean.\nCreate a new column called acct_amount_filled, which contains the values of acct_amount, except all NA values should be replaced with 5 times the amount in inv_amount.\nAssert that there are no missing values in the cust_id column of accounts_clean.\nAssert that there are no missing values in the acct_amount_filled column of accounts_clean.\n\n\n\nex_020.R\n\naccounts_clean <- accounts %>%\n  # Filter to remove rows with missing cust_id\n  filter(!is.na(cust_id))\n\n\naccounts_clean\n# Create accounts_clean\naccounts_clean <- accounts %>%\n  # Filter to remove rows with missing cust_id\n  filter(!is.na(cust_id)) %>%\n  # Add new col acct_amount_filled with replaced NAs\n  mutate(\n    acct_amount_filled = ifelse(\n      is.na(acct_amount), 5 * inv_amount, acct_amount))\n\naccounts_clean\n\n# Create accounts_clean\n# Create accounts_clean\naccounts_clean <- accounts %>%\n  # Filter to remove rows with missing cust_id\n  filter(!is.na(cust_id)) %>%\n  # Add new col acct_amount_filled with replaced NAs\n    mutate(\n        acct_amount_filled = ifelse(\n            is.na(acct_amount),\n            inv_amount * 5,\n            acct_amount\n        )\n    )\n# Assert that cust_id has no missing vals\nassert_all_are_not_na(accounts_clean$cust_id)\nassert_all_are_not_na(accounts_clean$acc)"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html",
    "href": "_cleaning_data_in_R_04.html",
    "title": "Instructions 100 XP",
    "section": "",
    "text": "ex_021.R"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#small-distance-small-difference",
    "href": "_cleaning_data_in_R_04.html#small-distance-small-difference",
    "title": "4  Record Linkage",
    "section": "4.1 Small distance, small difference",
    "text": "4.1 Small distance, small difference\nIn the video exercise, you learned that there are multiple ways to calculate how similar or different two strings are. Now you’ll practice using the stringdist package to compute string distances using various methods. It’s important to be familiar with different methods, as some methods work better on certain datasets, while others work better on other datasets.\nThe stringdist package has been loaded for you.\n\nInstructions 100 XP\n\nCalculate the Damerau-Levenshtein distance between “las angelos” and “los angeles”.\n\n\n\nex_021.R\n\n# Calculate Damerau-Levenshtein distance\nstringdist(\n    \"las angelos\",\n    \"los angeles\", method = \"dl\"\n)\n# Calculate LCS distance\nstringdist(\n    \"las angelos\",\n    \"los angeles\", method = \"lcs\"\n)\n\n# Calculate Jaccard distance\nstringdist(\n    \"las angelos\",\n    \"los angeles\", \n    method = \"jaccard\"\n)"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#section",
    "href": "_cleaning_data_in_R_04.html#section",
    "title": "4  Record Linkage",
    "section": "4.6 ",
    "text": "4.6 \n\nInstructions 100 XP\n\n\nex_026.R\n\n# Create pairs\npair_blocking(zagat, fodors, blocking_var = \"city\") %>%\n  # Compare pairs\n  compare_pairs(by = c(\"name\", \"addr\"), default_comparator = jaro_winkler()) %>%\n  # Score pairs\n  score_problink() %>%\n  # Select pairs\n  select_n_to_m() %>%\n  # Link data \n  link()"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#section-1",
    "href": "_cleaning_data_in_R_04.html#section-1",
    "title": "4  Record Linkage",
    "section": "4.6 ",
    "text": "4.6 \n\nInstructions 100 XP\n\n\nex_026.R"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#section-2",
    "href": "_cleaning_data_in_R_04.html#section-2",
    "title": "4  Record Linkage",
    "section": "4.7 ",
    "text": "4.7 \n\nInstructions 100 XP\n\n\nex_027.R"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#section-3",
    "href": "_cleaning_data_in_R_04.html#section-3",
    "title": "4  Record Linkage",
    "section": "4.7 ",
    "text": "4.7 \n\nInstructions 100 XP\n\n\nex_027.R"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#section-4",
    "href": "_cleaning_data_in_R_04.html#section-4",
    "title": "4  Record Linkage",
    "section": "4.7 ",
    "text": "4.7 \n\nInstructions 100 XP\n\n\nex_027.R"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#section-5",
    "href": "_cleaning_data_in_R_04.html#section-5",
    "title": "4  Record Linkage",
    "section": "4.7 ",
    "text": "4.7 \n\nInstructions 100 XP\n\n\nex_027.R"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#fixing-typos-with-string-distance",
    "href": "_cleaning_data_in_R_04.html#fixing-typos-with-string-distance",
    "title": "4  Record Linkage",
    "section": "4.2 Fixing typos with string distance",
    "text": "4.2 Fixing typos with string distance\nIn this chapter, one of the datasets you’ll be working with, zagat, is a set of restaurants in New York, Los Angeles, Atlanta, San Francisco, and Las Vegas. The data is from Zagat, a company that collects restaurant reviews, and includes the restaurant names, addresses, phone numbers, as well as other restaurant information.\nThe city column contains the name of the city that the restaurant is located in. However, there are a number of typos throughout the column. Your task is to map each city to one of the five correctly-spelled cities contained in the cities data frame.\nLoad dplyr, fuzzyjoin, zagat and cities.\n\nInstructions 100 XP\n\n\nex_022.R\n\n# Count the number of each city variation\n# Count the number of each city variation\nzagat %>%\n  count(city)\n\n# Join zagat and cities and look at results\nzagat %>%\n  # Left join based on stringdist using city and city_actual cols\n  stringdist_left_join(\n    cities,\n    by = c(\"city\" = \"city_actual\"), method = \"dl\"\n  )%>%\n  # Select the name, city, and city_actual cols\n  select(name, city, city_actual)"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#pair-blocking",
    "href": "_cleaning_data_in_R_04.html#pair-blocking",
    "title": "4  Record Linkage",
    "section": "4.3 Pair blocking",
    "text": "4.3 Pair blocking\nZagat and Fodor’s are both companies that gather restaurant reviews. The zagat and fodors datasets both contain information about various restaurants, including addresses, phone numbers, and cuisine types. Some restaurants appear in both datasets, but don’t necessarily have the same exact name or phone number written down. In this chapter, you’ll work towards figuring out which restaurants appear in both datasets.\nThe first step towards this goal is to generate pairs of records so that you can compare them. In this exercise, you’ll first generate all possible pairs, and then use your newly-cleaned city column as a blocking variable.\nMake zagat and fodors available.\n\nInstructions 100 XP\n\nLoad the reclin package.\nGenerate all possible pairs of records between the zagat and fodors datasets.\n\n\n\nex_023.R\n\n# Load reclin\nlibrary(reclin)\n# Generate all possible pairs\npair_blocking(zagat, fodors)\n\n# Generate pairs with same city\npair_blocking(zagat, fodors, blocking_var =\"city\")"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#comparing-pairs",
    "href": "_cleaning_data_in_R_04.html#comparing-pairs",
    "title": "4  Record Linkage",
    "section": "4.4 Comparing pairs",
    "text": "4.4 Comparing pairs\nNow that you’ve generated the pairs of restaurants, it’s time to compare them. You can easily customize how you perform your comparisons using the by and default_comparator arguments. There’s no right answer as to what each should be set to, so in this exercise, you’ll try a couple options out.\nLoad dplyr, reclin, zagat and fodors.\n\nInstructions 100 XP\n\nCompare pairs by name using lcs() distance.\n\n\n\nex_024.R\n\n# Generate pairs\npair_blocking(zagat, fodors, blocking_var = \"city\") %>%\n  # Compare pairs by name using lcs()\n  compare_pairs(\n    by = \"name\",\n    default_comparator = lcs()\n  )\n# Generate pairs\npair_blocking(zagat, fodors, blocking_var = \"city\") %>%\n  # Compare pairs by name, phone, addr\n  compare_pairs(\n    by = c(\"name\", \"phone\", \"addr\" ),\n    default_comparator = jaro_winkler()\n  )"
  },
  {
    "objectID": "_cleaning_data_in_R_04.html#putting-it-together",
    "href": "_cleaning_data_in_R_04.html#putting-it-together",
    "title": "4  Record Linkage",
    "section": "4.5 Putting it together",
    "text": "4.5 Putting it together\nDuring this chapter, you’ve cleaned up the city column of zagat using string similarity, as well as generated and compared pairs of restaurants from zagat and fodors. The end is near - all that’s left to do is score and select pairs and link the data together, and you’ll be able to begin your analysis in no time!\nLoad reclin, dplyr, zagat, and fodors.\n\nInstructions 100 XP\n\nScore the pairs of records probabilistically.\nSelect the pairs that are considered matches.\nLink the two data frames together.\n\n\n\nex_025.R"
  }
]