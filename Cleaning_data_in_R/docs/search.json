[
  {
    "objectID": "_cleaning_data_in_R_01.html#section",
    "href": "_cleaning_data_in_R_01.html#section",
    "title": "1  Common Data Problems",
    "section": "1.2 ",
    "text": "1.2 \n\nInstructions 100 XP\n\n\nex_002.R\n\n# Glimpse at bike_share_rides\nglimpse(bike_share_rides)\n\n# Summary of user_birth_year\nsummary(bike_share_rides$user_birth_year)\n\n# Convert user_birth_year to factor: user_birth_year_fct\nbike_share_rides <- bike_share_rides %>%\n  mutate(user_birth_year_fct = as.factor(user_birth_year))\n\n# Assert user_birth_year_fct is a factor\nassert_is_factor(bike_share_rides$user_birth_year_fct)\n\n# Summary of user_birth_year_fct\nsummary(bike_share_rides$user_birth_year_fct)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-1",
    "href": "_cleaning_data_in_R_01.html#section-1",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-2",
    "href": "_cleaning_data_in_R_01.html#section-2",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-3",
    "href": "_cleaning_data_in_R_01.html#section-3",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-4",
    "href": "_cleaning_data_in_R_01.html#section-4",
    "title": "1  Common Data Problems",
    "section": "1.7 ",
    "text": "1.7 \n\nInstructions 100 XP\n\n\nex_007.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-5",
    "href": "_cleaning_data_in_R_01.html#section-5",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-6",
    "href": "_cleaning_data_in_R_01.html#section-6",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-7",
    "href": "_cleaning_data_in_R_01.html#section-7",
    "title": "1  Common Data Problems",
    "section": "1.8 ",
    "text": "1.8 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#section-8",
    "href": "_cleaning_data_in_R_01.html#section-8",
    "title": "1  Common Data Problems",
    "section": "1.9 ",
    "text": "1.9 \n\nInstructions 100 XP\n\n\nex_008.R"
  },
  {
    "objectID": "intro.html#overcome-common-data-problems-like-removing-duplicates-in-r",
    "href": "intro.html#overcome-common-data-problems-like-removing-duplicates-in-r",
    "title": "Introduction",
    "section": "Overcome Common Data Problems Like Removing Duplicates in R",
    "text": "Overcome Common Data Problems Like Removing Duplicates in R\nIt’s commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions.\nIn this course, you’ll learn a variety of techniques to help you clean dirty data using R. You’ll start by converting data types, applying range constraints, and dealing with full and partial duplicates to avoid double-counting."
  },
  {
    "objectID": "intro.html#delve-into-advanced-data-challenges",
    "href": "intro.html#delve-into-advanced-data-challenges",
    "title": "Introduction",
    "section": "Delve into Advanced Data Challenges",
    "text": "Delve into Advanced Data Challenges\nOnce you’ve practiced working on common data issues, you’ll move on to more advanced challenges such as ensuring consistency in measurements and dealing with missing data. After every new concept, you’ll have the chance to complete a hands-on exercise to cement your knowledge and build your experience."
  },
  {
    "objectID": "intro.html#learn-to-use-record-linkage-during-data-cleaning",
    "href": "intro.html#learn-to-use-record-linkage-during-data-cleaning",
    "title": "Introduction",
    "section": "Learn to Use Record Linkage During Data Cleaning",
    "text": "Learn to Use Record Linkage During Data Cleaning\nRecord Linkage is used to merge datasets together when the values have issues such as typos or different spellings. You’ll explore this useful technique in the final chapter and practice the application by using it to join two restaurant review datasets together into a single dataset."
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#converting-data-types",
    "href": "_cleaning_data_in_R_01.html#converting-data-types",
    "title": "1  Common Data Problems",
    "section": "1.1 Converting data types",
    "text": "1.1 Converting data types\nThroughout this chapter, you’ll be working with San Francisco bike share ride data called bike_share_rides. It contains information on start and end stations of each trip, the trip duration, and some user information.\nBefore beginning to analyze any dataset, it’s important to take a look at the different types of columns you’ll be working with, which you can do using glimpse().\nIn this exercise, you’ll take a look at the data types contained in bike_share_rides and see how an incorrect data type can flaw your analysis.\ndplyr and assertive are loaded and bike_share_rides is available.\n\nInstructions 100 XP\n\nExamine the data types of the columns of bike_share_rides.\nGet a summary of the user_birth_year column of bike_share_rides.\n\n\n\nex_001.R\n\n# Glimpse at bike_share_rides\nglimpse(bike_share_rides)\n\n# Summary of user_birth_year\nsummary(bike_share_rides$user_birth_year)\n\n# Convert user_birth_year to factor: user_birth_year_fct\nbike_share_rides <- bike_share_rides %>%\n  mutate(user_birth_year_fct = as.factor(user_birth_year))\n\n# Assert user_birth_year_fct is a factor\nassert_is_factor(bike_share_rides$user_birth_year_fct)\n\n# Summary of user_birth_year_fct\nsummary(bike_share_rides$user_birth_year_fct)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#trimming-strings",
    "href": "_cleaning_data_in_R_01.html#trimming-strings",
    "title": "1  Common Data Problems",
    "section": "1.2 Trimming strings",
    "text": "1.2 Trimming strings\nIn the previous exercise, you were able to identify the correct data type and convert user_birth_year to the correct type, allowing you to extract counts that gave you a bit more insight into the dataset.\nAnother common dirty data problem is having extra bits like percent signs or periods in numbers, causing them to be read in as characters. In order to be able to crunch these numbers, the extra bits need to be removed and the numbers need to be converted from character to numeric. In this exercise, you’ll need to convert the duration column from character to numeric, but before this can happen, the word “minutes” needs to be removed from each value.\ndplyr, assertive, and stringr are loaded and bike_share_rides is available.\n\nInstructions 100 XP\n\nUse str_remove() to remove \"minutes\" from the duration column of bike_share_rides. Add this as a new column called duration_trimmed.\nConvert the duration_trimmed column to a numeric type and add this as a new column called duration_mins.\nGlimpse at bike_share_rides and assert that the duration_mins column is numeric.\nCalculate the mean of duration_mins.\n\n\n\nex_002.R\n\nbike_share_rides <- bike_share_rides %>%\n  # Remove 'minutes' from duration: duration_trimmed\n  mutate(duration_trimmed = str_remove(duration, \"minutes\"),\n  # Convert duration_trimmed to numeric: duration_mins\n         duration_mins = as.numeric(duration_trimmed))\n\n# Glimpse at bike_share_rides\nglimpse(bike_share_rides)\n\n# Assert duration_mins is numeric\nassert_is_numeric(bike_share_rides$duration_mins)\n\n# Calculate mean duration\nmean(bike_share_rides$duration_mins)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#ride-duration-constraints",
    "href": "_cleaning_data_in_R_01.html#ride-duration-constraints",
    "title": "1  Common Data Problems",
    "section": "1.3 Ride duration constraints",
    "text": "1.3 Ride duration constraints\nValues that are out of range can throw off an analysis, so it’s important to catch them early on. In this exercise, you’ll be examining the duration_min column more closely. Bikes are not allowed to be kept out for more than 24 hours, or 1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned.\nIn this exercise, you’ll replace erroneous data with the range limit ( 1440 minutes), however, you could just as easily replace these values with NAs.\nload dplyr, assertive, and ggplot2 andbike_share_rides.\n\nInstructions 100 XP\nCreate a three-bin histogram of the duration_min column of bike_share_rides using ggplot2 to identify if there is out-of-range data.\n\n\nex_003.R\n\n# Create breaks\nbreaks <- \n  c(\n    min(bike_share_rides$duration_min),\n    0,\n    1440,\n    max(bike_share_rides$duration_min)\n  )\n\n# Create a histogram of duration_min\nggplot(bike_share_rides, aes(duration_min)) +\n  geom_histogram(breaks = breaks)\n\n# duration_min_const: replace vals of duration_min > 1440 with 1440\nbike_share_rides <- bike_share_rides %>%\n  mutate(\n    duration_min_const = replace(\n      duration_min,\n      duration_min > 1440 ,\n      1440\n    )\n  )\n\n# Make sure all values of \n# duration_min_const are between 0 and 1440\nassert_all_are_in_closed_range(\n  bike_share_rides$duration_min_const,\n  lower = 0, upper = 1440\n)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#back-to-the-future",
    "href": "_cleaning_data_in_R_01.html#back-to-the-future",
    "title": "1  Common Data Problems",
    "section": "1.4 Back to the future",
    "text": "1.4 Back to the future\nSomething has gone wrong and it looks like you have data with dates from the future, which is way outside of the date range you expected to be working with. To fix this, you’ll need to remove any rides from the dataset that have a date in the future. Before you can do this, the date column needs to be converted from a character to a Date. Having these as Date objects will make it much easier to figure out which rides are from the future, since R makes it easy to check if one Date object is before (<) or after (>) another.\nload dplyr, assertive and bike_share_rides.\n\nInstructions 100 XP\n\nConvert the date column of bike_share_rides from character to the Date data type.\nAssert that all values in the date column happened sometime in the past and not in the future.\nFilter bike_share_rides to get only the rides from the past or today, and save this as bike_share_rides_past.\nAssert that the dates in bike_share_rides_past occurred only in the past.\n\n\n\nex_004.R\n\nlibrary(lubridate)\n# Convert date to Date type\nbike_share_rides <- bike_share_rides %>%\n  mutate(date = as.Date(date))\n\n# Make sure all dates are in the past\nassert_all_are_in_past(bike_share_rides$date)\n\n\n# Filter for rides that occurred before or on today's date\nbike_share_rides_past <- bike_share_rides %>%\n  filter(date <= today())\n\n# Make sure all dates from bike_share_rides_past are in the past\nassert_all_are_in_past(bike_share_rides_past$date)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#full-duplicates",
    "href": "_cleaning_data_in_R_01.html#full-duplicates",
    "title": "1  Common Data Problems",
    "section": "1.5 Full duplicates",
    "text": "1.5 Full duplicates\nYou’ve been notified that an update has been made to the bike sharing data pipeline to make it more efficient, but that duplicates are more likely to be generated as a result. To make sure that you can continue using the same scripts to run your weekly analyses about ride statistics, you’ll need to ensure that any duplicates in the dataset are removed first.\nWhen multiple rows of a data frame share the same values for all columns, they’re full duplicates of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Each ride, including its ride_id should be unique.\nbe sure thatdplyr is loaded and bike_share_rides is available.\n\nInstructions 100 XP\n\nGet the total number of full duplicates in bike_share_rides.\nRemove all full duplicates from bike_share_rides and save the new data frame as bike_share_rides_unique.\nGet the total number of full duplicates in the new bike_share_rides_unique data frame.\n\n\n\nex_005.R\n\n# Count the number of full duplicate\n\nsum(duplicated(bike_share_rides))\n\n# Remove duplicates\nbike_share_rides_unique <- distinct(bike_share_rides)\n\n# Count the full duplicates in bike_share_rides_unique\nsum(duplicated(bike_share_rides_unique))"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#full-duplicates-1",
    "href": "_cleaning_data_in_R_01.html#full-duplicates-1",
    "title": "1  Common Data Problems",
    "section": "1.6 Full duplicates",
    "text": "1.6 Full duplicates\nYou’ve been notified that an update has been made to the bike sharing data pipeline to make it more efficient, but that duplicates are more likely to be generated as a result. To make sure that you can continue using the same scripts to run your weekly analyses about ride statistics, you’ll need to ensure that any duplicates in the dataset are removed first.\nWhen multiple rows of a data frame share the same values for all columns, they’re full duplicates of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Each ride, including its ride_id should be unique.\nbe sure thatdplyr is loaded and bike_share_rides is available.\n\nInstructions 100 XP\n\nGet the total number of full duplicates in bike_share_rides.\nRemove all full duplicates from bike_share_rides and save the new data frame as bike_share_rides_unique.\nGet the total number of full duplicates in the new bike_share_rides_unique data frame.\n\n\n\nex_006.R\n\n# Count the number of full duplicate\nsum(duplicated(bike_share_rides))\n# Remove duplicates\nbike_share_rides_unique <- distinct(bike_share_rides)\n# Count the full duplicates in bike_share_rides_unique\nsum(duplicated(bike_share_rides_unique))"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#removing-partial-duplicates",
    "href": "_cleaning_data_in_R_01.html#removing-partial-duplicates",
    "title": "1  Common Data Problems",
    "section": "1.6 Removing partial duplicates",
    "text": "1.6 Removing partial duplicates\nNow that you’ve identified and removed the full duplicates, it’s time to check for partial duplicates. Partial duplicates are a bit tricker to deal with than full duplicates. In this exercise, you’ll first identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first.\ndplyr is loaded and bike_share_rides is available.\n\nInstructions 100 XP\n\nCount the number of occurrences of each ride_id.\nFilter for ride_ids that occur multiple times.\nRemove full and partial duplicates from bike_share_rides based on ride_id only, keeping all columns.\nStore this as bike_share_rides_unique.\n\n\n\nex_006.R\n\n# Find duplicated ride_ids\nbike_share_rides %>% \n  count(ride_id) %>% \n  filter(n > 1)\n\n# Remove full and partial duplicates\nbike_share_rides_unique <- bike_share_rides %>%\n  # Only based on ride_id instead of all cols\n  distinct(ride_id, .keep_all = TRUE)\n\n# Find duplicated ride_ids in bike_share_rides_unique\nbike_share_rides_unique %>%\n  # Count the number of occurrences of each ride_id\n  count(ride_id) %>%\n  # Filter for rows with a count > 1\n  filter(n>1)"
  },
  {
    "objectID": "_cleaning_data_in_R_01.html#aggregating-partial-duplicates",
    "href": "_cleaning_data_in_R_01.html#aggregating-partial-duplicates",
    "title": "1  Common Data Problems",
    "section": "1.7 Aggregating partial duplicates",
    "text": "1.7 Aggregating partial duplicates\nAnother way of handling partial duplicates is to compute a summary statistic of the values that differ between partial duplicates, such as mean, median, maximum, or minimum. This can come in handy when you’re not sure how your data was collected and want an average, or if based on domain knowledge, you’d rather have too high of an estimate than too low of an estimate (or vice versa).\ndplyr is loaded and bike_share_rides is available.\n\n1.7.1 Instructions 100 XP\n\nGroup bike_share_rides by ride_id and date.\nAdd a column called duration_min_avg that contains the mean ride duration for the row’s ride_id and date.\nRemove duplicates based on ride_id and date, keeping all columns of the data frame.\nRemove the duration_min column.\n\n\n\nex_007.R\n\nbike_share_rides %>%\n  # Group by ride_id and date\n  group_by(ride_id, date) %>%\n  # Add duration_min_avg column\n  mutate(duration_min_avg = mean(duration_min)) %>%\n  # Remove duplicates based on ride_id and date, keep all cols\n  distinct(ride_id, date, .keep_all = TRUE) %>%\n  # Remove duration_min column\n  select(-duration_min)"
  },
  {
    "objectID": "_cleaning_data_in_R_02.html#section",
    "href": "_cleaning_data_in_R_02.html#section",
    "title": "2  Categorical and Text Data",
    "section": "2.1 ",
    "text": "2.1 \n\n2.1.1 Instructions 100 XP\n{.r filename= ex_}"
  }
]