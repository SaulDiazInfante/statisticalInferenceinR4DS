[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this course, you will take a deeper dive into the wide range of data formats out there. More specifically, you’ll learn how to import data from relational databases and how to import and work with data coming from the web. Finally, you’ll get hands-on experience with importing data from statistical software packages such as SAS, STATA, and SPSS."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intermediate Importing Data in R",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "_intermediate_importing_data_in_R_01.html#establish-a-connection",
    "href": "_intermediate_importing_data_in_R_01.html#establish-a-connection",
    "title": "1  Importing data from databases (Part 1)",
    "section": "1.1 Establish a connection",
    "text": "1.1 Establish a connection\nThe first step to import data from a SQL database is creating a connection to it. As Filip explained, you need different packages depending on the database you want to connect to. All of these packages do this in a uniform way, as specified in the DBI package.\ndbConnect() creates a connection between your R session and a SQL database. The first argument has to be a DBIdriver object, that specifies how connections are made and how data is mapped between R and the database. Specifically for MySQL databases, you can build such a driver with RMySQL::MySQL().\nIf the MySQL database is a remote database hosted on a server, you’ll also have to specify the following arguments in dbConnect(): dbname, host, port, user and password. Most of these details have already been provided.\n\nInstructions 100 XP\n\nLoad the DBI library, which is already installed on DataCamp’s servers.\nEdit the dbConnect() call to connect to the MySQL database. Change the port argument (3306) and user argument (\"student\").\n\n\n\nex_001.R\n\n# Load the DBI package\nlibrary(DBI)\n\n# Edit dbConnect() call\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = \"tweater\", \n                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\", \n                 port = 3306,\n                 user = \"student\",\n                 password = \"datacamp\")"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_01.html#list-the-database-tables",
    "href": "_intermediate_importing_data_in_R_01.html#list-the-database-tables",
    "title": "1  Importing data from databases (Part 1)",
    "section": "1.2 List the database tables",
    "text": "1.2 List the database tables\nAfter you’ve successfully connected to a remote MySQL database, the next step is to see what tables the database contains. You can do this with the dbListTables() function. As you might remember from the video, this function requires the connection object as an input, and outputs a character vector with the table names.\n\nInstructions 100 XP\n\nAdd code to create a vector tables, that contains the tables in the tweater database. You can connect to this database through the con object.\nDisplay the structure of tables; what’s the class of this vector?\n\n\n\nex_002.R\n\n# Load the DBI package\nlibrary(DBI)\n# Connect to the MySQL database: con\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = \"tweater\", \n                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\", \n                 port = 3306,\n                 user = \"student\",\n                 password = \"datacamp\")\n# Build a vector of table names: tables\ntables <- dbListTables(con)\n# Display structure of tables\nstr(tables)\nclass(tables)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_01.html#import-users",
    "href": "_intermediate_importing_data_in_R_01.html#import-users",
    "title": "1  Importing data from databases (Part 1)",
    "section": "1.3 Import users",
    "text": "1.3 Import users\nAs you might have guessed by now, the database contains data on a more tasty version of Twitter, namely Tweater. Users can post tweats with short recipes for delicious snacks. People can comment on these tweats. There are three tables: users, tweats, and comments that have relations among them. Which ones, you ask? You’ll discover in a moment!\nLet’s start by importing the data on the users into your R session. You do this with the ‘dbReadTable()’ function. Simply pass it the connection object (‘con’), followed by the name of the table you want to import. The resulting object is a standard R data frame.\n\nInstructions 100 XP\n\nAdd code that imports the “users” table from the tweater database and store the resulting data frame as users.\nPrint the users data frame.\n\n\n\nex_003.R\n\n# Load the DBI package\nlibrary(DBI)\n\n# Connect to the MySQL database: con\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = \"tweater\", \n                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\", \n                 port = 3306,\n                 user = \"student\",\n                 password = \"datacamp\")\n\n# Import the users table from tweater: users\nusers <- dbReadTable(con, \"users\")\n\n# Print users\nusers"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_01.html#import-all-tables",
    "href": "_intermediate_importing_data_in_R_01.html#import-all-tables",
    "title": "1  Importing data from databases (Part 1)",
    "section": "1.4 Import all tables",
    "text": "1.4 Import all tables\nNext to the users, we’re also interested in the tweats and comments tables. However, separate dbReadTable() calls for each and every one of the tables in your database would mean a lot of code duplication. Remember about the lapply() function? You can use it again here! A connection is already coded for you, as well as a vector table_names, containing the names of all the tables in the database. ### Instructions 100 XP {.unnumbered}\n\nFinish the lapply() function to import the users, tweats and comments tables in a single call. The result, a list of data frames, will be stored in the variable tables.\nPrint tables to check if you got it right.\n\n\n\nex_004.R\n\n# Load the DBI package\nlibrary(DBI)\n\n# Connect to the MySQL database: con\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = \"tweater\", \n                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\", \n                 port = 3306,\n                 user = \"student\",\n                 password = \"datacamp\")\n\n# Get table names\ntable_names <- dbListTables(con)\n\n# Import all tables\ntables <- lapply(table_names, dbReadTable, conn = con)\n\n# Print out tables\ntables"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_02.html#query-tweater-1",
    "href": "_intermediate_importing_data_in_R_02.html#query-tweater-1",
    "title": "2  Importing data from databases (Part 2)",
    "section": "2.1 Query tweater (1)",
    "text": "2.1 Query tweater (1)\nIn your life as a data scientist, you’ll often be working with huge databases that contain tables with millions of rows. If you want to do some analyses on this data, it’s possible that you only need a fraction of this data. In this case, it’s a good idea to send SQL queries to your database, and only import the data you actually need into R.\ndbGetQuery() is what you need. As usual, you first pass the connection object to it. The second argument is an SQL query in the form of a character string. This example selects the age variable from the people dataset where gender equals “male”:\ndbGetQuery(con, \"SELECT age FROM people WHERE gender = 'male'\")\nA connection to the tweater database has already been coded for you.\n\nInstructions 100 XP\n\nUse dbGetQuery() to create a data frame, elisabeth, that selects the tweat_id column from the comments table where elisabeth is the commenter, heruser_id is 1\nPrint out elisabeth so you can see if you queried the database correctly.\n\n\n\nex_005.R\n\n# Connect to the database\nlibrary(DBI)\ncon <- dbConnect(RMySQL::MySQL(),\n    dbname = \"tweater\",\n    host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n    port = 3306,\n    user = \"student\",\n    password = \"datacamp\"\n)\n\n# Import tweat_id column of comments where user_id is 1: elisabeth\n\nqry <- \"SELECT tweat_id FROM comments WHERE user_id = 1\"\nelisabeth <- dbGetQuery(con, qry)\n\n# Print elisabeth\nelisabeth"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_02.html#query-tweater-2",
    "href": "_intermediate_importing_data_in_R_02.html#query-tweater-2",
    "title": "2  Importing data from databases (Part 2)",
    "section": "2.2 Query tweater (2)",
    "text": "2.2 Query tweater (2)\nApart from checking equality, you can also check for less than and greater than relationships, with < and >, just like in R.\ncon, a connection to the tweater database, is again available.\n\nInstructions 100 XP\n\nCreate a data frame, latest, that selects the post column from the tweats table observations where the date is higher than '2015-09-21'.\nPrint out latest.\n\n\n\nex_006.R\n\n# Connect to the database\nlibrary(DBI)\ncon <- dbConnect(RMySQL::MySQL(),\n                 dbname = \"tweater\",\n                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n                 port = 3306,\n                 user = \"student\",\n                 password = \"datacamp\")\n\n# Import post column of tweats where date is higher than '2015-09-21': latest\n\nqry <- \n    \"SELECT post FROM tweats WHERE date > '2015-09-21'\"\n\nlatest <- dbGetQuery(con, qry)\n# Print latest\nlatest"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_02.html#query-tweater-3",
    "href": "_intermediate_importing_data_in_R_02.html#query-tweater-3",
    "title": "2  Importing data from databases (Part 2)",
    "section": "2.3 Query tweater (3)",
    "text": "2.3 Query tweater (3)\nSuppose that you have a people table, with a bunch of information. This time, you want to find out the age and country of married males. Provided that there is a married column that’s 1 when the person in question is married, the following query would work.\nSELECT age, country\n  FROM people\n    WHERE gender = \"male\" AND married = 1\nCan you use a similar approach for a more specialized query on the tweater database?\n\nInstructions 100 XP\nCreate an R data frame, specific, that selects the message column from the comments table where the tweat_id is 77 and the user_id is greater than 4. Print specific.\n\n\nex_007.R\n\n# Connect to the database\nlibrary(DBI)\ncon <- dbConnect(RMySQL::MySQL(),\n                 dbname = \"tweater\",\n                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n                 port = 3306,\n                 user = \"student\",\n                 password = \"datacamp\")\n\n# Create data frame specific\nqry <- \n    \"SELECT message \n        FROM comments\n            WHERE tweat_id = 77 AND user_id > 4\"\nspecific <- dbGetQuery(con, qry)\n\n# Print specific\nspecific"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_02.html#query-tweater-4",
    "href": "_intermediate_importing_data_in_R_02.html#query-tweater-4",
    "title": "2  Importing data from databases (Part 2)",
    "section": "2.4 Query tweater (4)",
    "text": "2.4 Query tweater (4)\nThere are also dedicated SQL functions that you can use in the WHERE clause of an SQL query. For example, CHAR_LENGTH() returns the number of characters in a string.\n\nInstructions 100 XP\n\nCreate a data frame, short, that selects the id and name columns from the users table where the number of characters in the name is strictly less than 5.\nPrint short.\n\n\n\nex_008.R\n\n# Connect to the database\nlibrary(DBI)\ncon <- dbConnect(RMySQL::MySQL(),\n                 dbname = \"tweater\",\n                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n                 port = 3306,\n                 user = \"student\",\n                 password = \"datacamp\")\n\n# Create data frame short\nqry <- \n    \"SELECT id, name \n        FROM users \n            WHERE CHAR_LENGTH(name) < 5\n    \"\nshort <- dbGetQuery(con, qry)\n\n# Print short\nshort"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_02.html#send---fetch---clear",
    "href": "_intermediate_importing_data_in_R_02.html#send---fetch---clear",
    "title": "2  Importing data from databases (Part 2)",
    "section": "2.5 Send - Fetch - Clear",
    "text": "2.5 Send - Fetch - Clear\nYou’ve used dbGetQuery() multiple times now. This is a virtual function from the DBI package, but is actually implemented by the RMySQL package. Behind the scenes, the following steps are performed:\n\nSending the specified query with dbSendQuery();\nFetching the result of executing the query on the database with dbFetch();\nClearing the result with dbClearResult().\n\nLet’s not use dbGetQuery() this time and implement the steps above. This is tedious to write, but it gives you the ability to fetch the query’s result in chunks rather than all at once. You can do this by specifying the n argument inside dbFetch().\n\nInstructions 100 XP\n\nInspect the dbSendQuery() call that has already been coded for you. It selects the comments for the users with an id above 4.\nUse dbFetch() twice. In the first call, import only two records of the query result by setting the n argument to 2. In the second call, import all remaining queries (don’t specify n). In both calls, simply print the resulting data frames.\nClear res with dbClearResult().\n\n\n\nex_009.R\n\n# Connect to the database\nlibrary(DBI)\ncon <- dbConnect(RMySQL::MySQL(),\n                 dbname = \"tweater\",\n                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n                 port = 3306,\n                 user = \"student\",\n                 password = \"datacamp\")\n\n# Send query to the database\nres <- dbSendQuery(con, \"SELECT * FROM comments WHERE user_id > 4\")\n\n# Use dbFetch() twice\n\ndbFetch(res, n = 2)\ndbFetch(res)\n\n# Clear res\ndbClearResult(res)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_02.html#be-polite-and",
    "href": "_intermediate_importing_data_in_R_02.html#be-polite-and",
    "title": "2  Importing data from databases (Part 2)",
    "section": "2.6 Be polite and …",
    "text": "2.6 Be polite and …\nEvery time you connect to a database using dbConnect(), you’re creating a new connection to the database you’re referencing. RMySQL automatically specifies a maximum of open connections and closes some of the connections for you, but still: it’s always polite to manually disconnect from the database afterwards. You do this with the dbDisconnect() function.\nThe code that connects you to the database is already available, can you finish the script?\n\nInstructions 100 XP\n\nUsing the technique you prefer, build a data frame long_tweats. It selects the post and date columns from the observations in tweats where the character length of the post variable exceeds 40.\nPrint long_tweats.\nDisconnect from the database by using dbDisconnect().\n\n\n\nex_010.R\n\n# Connect to the database\nlibrary(DBI)\ncon <- dbConnect(RMySQL::MySQL(),\n                 dbname = \"tweater\",\n                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n                 port = 3306,\n                 user = \"student\",\n                 password = \"datacamp\")\n\n# Create the data frame  long_tweats\nqry <- \n    \"SELECT post, date\n        FROM tweats\n            WHERE CHAR_LENGTH(post) > 40\n    \"\nlong_tweats <- dbGetQuery(con, qry)\n# Print long_tweats\nprint(long_tweats)\n\n# Disconnect from the database\ndbDisconnect(con)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_03.html#import-flat-files-from-the-web",
    "href": "_intermediate_importing_data_in_R_03.html#import-flat-files-from-the-web",
    "title": "3  Importing data from the web (Part 1)",
    "section": "3.1 Import flat files from the web",
    "text": "3.1 Import flat files from the web\nThe utils functions to import flat file data, such as read.csv() and read.delim(), are capable of automatically importing from URLs that point to flat files on the web.\nYou must be wondering whether Hadley Wickham’s alternative package, readr, is equally potent. Well, figure it out in this exercise! The URLs for both a .csv file as well as a .delim file are already coded for you. It’s up to you to actually import the data. If it works, that is…\n\nInstructions 100 XP\n\nLoad the readr package. It’s already installed on DataCamp’s servers.\nUse url_csv to read in the .csv file it is pointing to. Use the read_csv() function. The .csv contains column names in the first row. Save the resulting data frame as pools.\nSimilarly, use url_delim to read in the online .txt file. Use the read_tsv() function and store the result as potatoes.\nPrint pools and potatoes. Looks correct?\n\n\n\nex_011.R\n\nlibrary(\"readr\")\n\n# Import the csv file: pools\nurl_csv <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv\"\n\npools <- read_csv(url_csv)\n\n# Import the txt file: potatoes\nurl_delim <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt\"\npotatoes <- read_tsv(url_delim)\n\n# Print pools and potatoes\npools\npotatoes"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_03.html#secure-importing",
    "href": "_intermediate_importing_data_in_R_03.html#secure-importing",
    "title": "3  Importing data from the web (Part 1)",
    "section": "3.2 Secure importing",
    "text": "3.2 Secure importing\nIn the previous exercises, you have been working with URLs that all start with http://. There is, however, a safer alternative to HTTP, namely HTTPS, which stands for HyperText Transfer Protocol Secure. Just remember this: HTTPS is relatively safe, HTTP is not.\nLuckily for us, you can use the standard importing functions with https:// connections since R version 3.2.2.\n\nInstructions 100 XP\n\nTake a look at the URL in url_csv. It uses a secure connection, https://.\nUse read.csv() to import the file at url_csv. The .csv file it is referring to contains column names in the first row. Call it pools1.\nLoad the readr package.\nUse read_csv() to read in the same .csv file in url_csv. Call it pools2.\nPrint out the structure of pools1 and pools2. Looks like the importing went equally well as with a normal http connection!\n\n\n\nex_012.R\n\n# https URL to the swimming_pools csv file.\nurl_csv <-\n\"https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/\ndatasets/swimming_pools.csv\n\"\n\n# Import the file using read.csv(): pools1\npools1 <- read.csv(url_csv)\n\n# Load the readr package\nlibrary(readr)\n\n# Import the file using read_csv(): pools2\npools2 <- read_csv(url_csv)\n\n# Print the structure of pools1 and pools2\nstr(pools1)\nstr(pools2)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_03.html#importing-excel-files-from-the-web",
    "href": "_intermediate_importing_data_in_R_03.html#importing-excel-files-from-the-web",
    "title": "3  Importing data from the web (Part 1)",
    "section": "3.3 Importing Excel files from the web",
    "text": "3.3 Importing Excel files from the web\nWhen you learned about gdata, it was already mentioned that gdata can handle .xls files that are on the internet. readxl can’t, at least not yet. The URL with which you’ll be working is already available in the sample code. You will import it once using gdata and once with the readxl package via a workaround.\n\nInstructions 100 XP\n\nLoad the readxl and gdata packages.\nImport the .xls file located at the URL url_xls using read.xls() from gdata.\n\nStore the resulting data frame as excel_gdata.\nYou can not use read_excel() directly with a URL. Complete the following instructions to work around this problem:\nUse download.file() to download the .xls file behind the URL and store it locally as “local_latitude.xls”.\nCall read_excel() to import the local file, “local_latitude.xls”. Name the resulting data frame excel_readxl.\n\n\n\nex_013.R\n\n# Load the readxl and gdata package\nlibrary(readxl)\nlibrary(gdata)\n\n\n# Specification of url: url_xls\nurl_xls <- \n\"http://s3.amazonaws.com/assets.datacamp.com/production/\ncourse_1478/datasets/latitude.xls\"\n\n# Import the .xls file with gdata: excel_gdata\nexcel_gdata <- read.xls(url_xls)\n\n# Download file behind URL, name it local_latitude.xls\ndest_path <- file.path(\"~\", \"local_latitude.xls\")\ndownload.file(url_xls, \"local_latitude.xls\")\n\n# Import the local .xls file with readxl: excel_readxl\nexcel_readxl <- read_excel(\"local_latitude.xls\")"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_03.html#donwloading-any-file-secure-or-not",
    "href": "_intermediate_importing_data_in_R_03.html#donwloading-any-file-secure-or-not",
    "title": "3  Importing data from the web (Part 1)",
    "section": "3.4 Donwloading any file, secure or not",
    "text": "3.4 Donwloading any file, secure or not\nIn the previous exercise you’ve seen how you can read excel files on the web using the read_excel package by first downloading the file with the download.file() function.\nThere’s more: with download.file() you can download any kind of file from the web, using HTTP and HTTPS: images, executable files, but also .RData files. An RData file is very efficient format to store R data.\nYou can load data from an RData file using the load() function, but this function does not accept a URL string as an argument. In this exercise, you’ll first download the RData file securely, and then import the local data file.\n\nInstructions 100 XP\n\nTake a look at the URL in url_rdata. It uses a secure connection, https://. This URL points to an RData file containing a data frame with some metrics on different kinds of wine.\nDownload the file aturl_rdata using download.file(). Call the file \"wine_local.RData\" in your working directory.\nLoad the file you created, wine_local.RData, using the load() function. It takes one argument, the path to the file, which is just the filename in our case. After running this command, the variable wine will automatically be available in your workspace.\nPrint out the summary() of the wine dataset.\n\n\n\nex_014.R\n\n# https URL to the wine RData file.\nurl_rdata <- \n\"https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData\"\n\n# Download the wine file to your working directory\ndownload.file(url_rdata, \"wine_local.RData\")\n\n# Load the wine data into your workspace using load()\nload(\"wine_local.RData\")\n\n# Print out the summary of the wine data\nsummary(wine)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_03.html#http-httr-1",
    "href": "_intermediate_importing_data_in_R_03.html#http-httr-1",
    "title": "3  Importing data from the web (Part 1)",
    "section": "3.5 HTTP? httr! (1)",
    "text": "3.5 HTTP? httr! (1)\nDownloading a file from the Internet means sending a GET request and receiving the file you asked for. Internally, all the previously discussed functions use a GET request to download files.\nhttr provides a convenient function, GET() to execute this GET request. The result is a response object, that provides easy access to the status code, content-type and, of course, the actual content.\nYou can extract the content from the request using the content() function. At the time of writing, there are three ways to retrieve this content: as a raw object, as a character vector, or an R object, such as a list. If you don’t tell content() how to retrieve the content through the as argument, it’ll try its best to figure out which type is most appropriate based on the content-type.\n\nInstructions 100 XP\n\nLoad the httr package. It’s already installed on DataCamp’s servers.\nUse GET() to get the URL stored in url. Store the result of this GET() call as resp.\nPrint the resp object. What information does it contain?\nGet the content of resp using content() and set the as argument to “raw”. Assign the resulting vector to raw_content.\nPrint the first values in raw_content with head().\n\n\n\nex_015.R\n\n# Load the httr package\nlibrary(httr)\n\n# Get the url, save response to resp\nurl <- \"http://www.example.com/\"\n\nresp <- GET(url)\n# Print resp\nprint(resp)\n\n# Get the raw content of resp: raw_content\nraw_content <- content(resp, as = 'raw')\n\n# Print the head of raw_content\nhead(raw_content)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_03.html#http-httr-2",
    "href": "_intermediate_importing_data_in_R_03.html#http-httr-2",
    "title": "3  Importing data from the web (Part 1)",
    "section": "3.6 HTTP? httr! (2)",
    "text": "3.6 HTTP? httr! (2)\nWeb content does not limit itself to HTML pages and files stored on remote servers such as DataCamp’s Amazon S3 instances. There are many other data formats out there. A very common one is JSON. This format is very often used by so-called Web APIs, interfaces to web servers with which you as a client can communicate to get or store information in more complicated ways.\nYou’ll learn about Web APIs and JSON in the video and exercises that follow, but some experimentation never hurts, does it?\n\nInstructions 100 XP\n\nUse GET() to get the url that has already been specified in the sample code. Store the response as resp.\nPrint resp. What is the content-type?\nUse content() to get the content of resp. Set the as argument to \"text\". Simply print out the result. What do you see?\nUse content() to get the content of resp, but this time do not specify a second argument. R figures out automatically that you’re dealing with a JSON, and converts the JSON to a named R list.\n\n\n\nex_016.R\n\n# httr is already loaded\n# Get the url\nurl <- \"http://www.omdbapi.com/?\napikey=72bc447a&t=Annie+Hall&y=&plot=short&r=json\"\nresp <- GET(url)\n# Print resp\nprint(resp)\n# Print content of resp as text\nprint(content(resp, as = \"text\"))\n# Print content of resp\nprint(content(resp))"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html#from-json-to-r",
    "href": "_intermediate_importing_data_in_R_04.html#from-json-to-r",
    "title": "4  Importing data from the web (Part 2)",
    "section": "4.1 From JSON to R",
    "text": "4.1 From JSON to R\nIn the simplest setting, fromJSON() can convert character strings that represent JSON data into a nicely structured R list. Give it a try!\n\nInstructions 100 XP\n\nLoad the jsonlite package. It’s already installed on DataCamp’s servers.\nwine_json represents a JSON. Use fromJSON() to convert it to a list, named wine.\nDisplay the structure of wine\n\n\n\nex_017.R\n\n# Load the jsonlite package\nlibrary(jsonlite)\n\n# wine_json is a JSON\nwine_json <- '\n    {\n        \"name\":\"Chateau Migraine\",\n        \"year\":1997,\n        \"alcohol_pct\":12.4,\n        \"color\":\"red\",\n        \"awarded\":false\n    }\n'\n\n# Convert wine_json into a list: wine\nwine <- fromJSON(wine_json)\n\n# Print structure of wine\nprint(str(wine))"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html#quandl-api",
    "href": "_intermediate_importing_data_in_R_04.html#quandl-api",
    "title": "4  Importing data from the web (Part 2)",
    "section": "4.2 Quandl API",
    "text": "4.2 Quandl API\nAs Filip showed in the video, fromJSON() also works if you pass a URL as a character string or the path to a local file that contains JSON data. Let’s try this out on the Quandl API, where you can fetch all sorts of financial and economical data.\n\nInstructions 100 XP\n\nquandl_url represents a URL. Use fromJSON() directly on this URL and store the result in quandl_data.\nDisplay the structure of quandl_data .\n\n\n\nex_018.R\n\n# jsonlite is preloaded\n\n# Definition of quandl_url\nquandl_url <- \"https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?auth_token=i83asDsiWUUyfoypkgMz\"\n\n# Import Quandl data: quandl_data\nquandl_data <- fromJSON(quandl_url)\n\n# Print structure of quandl_data\nstr(quandl_data)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html",
    "href": "_intermediate_importing_data_in_R_04.html",
    "title": "4  Importing data from the web (Part 2)",
    "section": "",
    "text": "5 OMDb API\nIn the video, you saw how easy it is to interact with an API once you know how to formulate requests. You also saw how to fetch all information on Rain Man from OMDb. Simply perform a GET() call, and next ask for the contents with the content() function. This content() function, which is part of the httr package, uses jsonlite behind the scenes to import the JSON data into R.\nHowever, by now you also know that jsonlite can handle URLs itself. Simply passing the request URL to fromJSON() will get your data into R. In this exercise, you will be using this technique to compare the release year of two movies in the Open Movie Database."
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html#instructions-100-xp-2",
    "href": "_intermediate_importing_data_in_R_04.html#instructions-100-xp-2",
    "title": "4  Importing data from the web (Part 2)",
    "section": "Instructions 100 XP",
    "text": "Instructions 100 XP\n\nTwo URLs are included in the sample code, as well as a fromJSON() call to build sw4. Add a similar call to build sw3.\nPrint out the element named Title of both sw4 and sw3. You can use the $ operator. What movies are we dealing with here? -Write an expression that evaluates to TRUE if sw4 was released later than sw3. This information is stored in the Year element of the named lists.\n\n\n\nex_019.R\n\n# The package jsonlite is already loaded\n\n# Definition of the URLs\nurl_sw4 <- \"http://www.omdbapi.com/?apikey=72bc447a&i=tt0076759&r=json\"\nurl_sw3 <- \"http://www.omdbapi.com/?apikey=72bc447a&i=tt0121766&r=json\"\n\n# Import two URLs with fromJSON(): sw4 and sw3\nsw3 <- fromJSON(url_sw3)\nsw4 <- fromJSON(url_sw4)\n\n\n# Print out the Title element of both lists\nprint(sw3$Title)\nprint(sw4$Title)\n\n# Is the release year of sw4 later than sw3?\nsw4$Year > sw3$Year"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html#json-practice-1",
    "href": "_intermediate_importing_data_in_R_04.html#json-practice-1",
    "title": "4  Importing data from the web (Part 2)",
    "section": "4.4 JSON practice (1)",
    "text": "4.4 JSON practice (1)\nJSON is built on two structures: objects and arrays. To help you experiment with these, two JSON strings are included in the sample code. It’s up to you to change them appropriately and then call jsonlite’s fromJSON() function on them each time.\n\nInstrucions 100 XP\n\nChange the assignment of json1 such that the R vector after conversion contains the numbers 1 up to 6, in ascending order. Next, call fromJSON() on json1.\nAdapt the code for json2 such that it’s converted to a named list with two elements: a, containing the numbers 1, 2 and 3 and b, containing the numbers 4, 5 and 6. Next, call fromJSON() on json2.\n\n\n\nex_020.R\n\n# jsonlite is already loaded\n# Challenge 1\njson1 <- '[1, 2, 3, 4, 5, 6]'\nfromJSON(json1)\n# Challenge 2\njson2 <- '{\n    \"a\": [1, 2, 3],\n    \"b\": [4, 5, 6]\n}'\nfromJSON(json2)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html#section",
    "href": "_intermediate_importing_data_in_R_04.html#section",
    "title": "4  Importing data from the web (Part 2)",
    "section": "5.3 ",
    "text": "5.3 \n\n5.3.1 Instrucions 100 XP"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html#json-practice-2",
    "href": "_intermediate_importing_data_in_R_04.html#json-practice-2",
    "title": "4  Importing data from the web (Part 2)",
    "section": "4.5 JSON practice (2)",
    "text": "4.5 JSON practice (2)\nWe prepared two more JSON strings in the sample code. Can you change them and call jsonlite’s fromJSON() function on them, similar to the previous exercise?\n\nInstrucions 100 XP\n\nRemove characters from json1 to build a 2 by 2 matrix containing only 1, 2, 3 and 4. Call fromJSON() on json1.\nAdd characters to json2 such that the data frame in which the json is converted contains an additional observation in the last row. For this observations, a equals 5 and b equals 6. Call fromJSON() one last time, on json2."
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html#tojson",
    "href": "_intermediate_importing_data_in_R_04.html#tojson",
    "title": "4  Importing data from the web (Part 2)",
    "section": "4.7 toJSON()",
    "text": "4.7 toJSON()\nApart from converting JSON to R with fromJSON(), you can also use toJSON() to convert R data to a JSON format. In its most basic use, you simply pass this function an R object to convert to a JSON. The result is an R object of the class json, which is basically a character string representing that JSON.\nFor this exercise, you will be working with a .csv file containing information on the amount of desalinated water that is produced around the world. As you’ll see, it contains a lot of missing values. This data can be found on the URL that is specified in the sample code.\n\nInstrucions 100 XP\n\nUse a function of the utils package to import the .csv file directly from the URL specified in url_csv. Save the resulting data frame as water. Make sure that strings are not imported as factors.\nConvert the data frame water to a JSON. Call the resulting object water_json.\nPrint out water_json.\n\n\n\nex_021.R\n\n# jsonlite is already loaded\n\n# URL pointing to the .csv file\nurl_csv <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv\"\n\n# Import the .csv file located at url_csv\nwater <- read.csv(url_csv)\n\n# Convert the data file according to the requirements\nwater_json <- toJSON(water)\n\n# Print out water_json\nprint(water_json)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html#minify-and-prettify",
    "href": "_intermediate_importing_data_in_R_04.html#minify-and-prettify",
    "title": "4  Importing data from the web (Part 2)",
    "section": "4.8 Minify and prettify",
    "text": "4.8 Minify and prettify\nJSONs can come in different formats. Take these two JSONs, that are in fact exactly the same: the first one is in a minified format, the second one is in a pretty format with indentation, whitespace and new lines:\n# Mini\n{\"a\":1,\"b\":2,\"c\":{\"x\":5,\"y\":6}}\n\n# Pretty\n{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": {\n    \"x\": 5,\n    \"y\": 6\n  }\n}\nUnless you’re a computer, you surely prefer the second version. However, the standard form that toJSON() returns, is the minified version, as it is more concise. You can adapt this behavior by setting the pretty argument inside toJSON() to TRUE. If you already have a JSON string, you can use prettify() or minify() to make the JSON pretty or as concise as possible.\n\nInstrucions 100 XP\n\nConvert the mtcars dataset, which is available in R by default, to a pretty JSON. Call the resulting JSON pretty_json.\nPrint out pretty_json. Can you understand the output easily?\nConvert pretty_json to a minimal version using minify(). Store this version under a new variable, mini_json.\nPrint out mini_json. Which version do you prefer, the pretty one or the minified one?\n\n\n\nex_022.R\n\n# jsonlite is already loaded\n# Convert mtcars to a pretty JSON: pretty_json\npretty_json <- toJSON(mtcars, pretty = TRUE)\n# Print pretty_json\nprint(pretty_json)\n\n# Minify pretty_json: mini_json\n\nmini_json <- minify(pretty_json)\n# Print mini_json\nprint(mini_json)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html#json-practice-2-1",
    "href": "_intermediate_importing_data_in_R_04.html#json-practice-2-1",
    "title": "4  Importing data from the web (Part 2)",
    "section": "4.6 JSON practice (2)",
    "text": "4.6 JSON practice (2)\n\nInstrucions 100 XP\n\nRemove characters from json1 to build a 2 by 2 matrix containing only 1, 2, 3 and 4. Call fromJSON() on json1.\nAdd characters to json2 such that the data frame in which the json is converted contains an additional observation in the last row. For this observations, a equals 5 and b equals 6. Call fromJSON() one last time, on json2.\n\n\n\nex_021.R\n\n# jsonlite is already loaded\n# Challenge 1\njson1 <- '[[1, 2], [3, 4]]'\nfromJSON(json1)\n\n# Challenge 2\njson2 <-\n   '[{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}, {\"a\": 5, \"b\": 6}\n]'\nfromJSON(json2)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_04.html#omdb-api",
    "href": "_intermediate_importing_data_in_R_04.html#omdb-api",
    "title": "4  Importing data from the web (Part 2)",
    "section": "4.3 OMDb API",
    "text": "4.3 OMDb API\nIn the video, you saw how easy it is to interact with an API once you know how to formulate requests. You also saw how to fetch all information on Rain Man from OMDb. Simply perform a GET() call, and next ask for the contents with the content() function. This content() function, which is part of the httr package, uses jsonlite behind the scenes to import the JSON data into R.\nHowever, by now you also know that jsonlite can handle URLs itself. Simply passing the request URL to fromJSON() will get your data into R. In this exercise, you will be using this technique to compare the release year of two movies in the Open Movie Database.\n\nInstructions 100 XP\n\nTwo URLs are included in the sample code, as well as a fromJSON() call to build sw4. Add a similar call to build sw3.\nPrint out the element named Title of both sw4 and sw3. You can use the $ operator. What movies are we dealing with here? -Write an expression that evaluates to TRUE if sw4 was released later than sw3. This information is stored in the Year element of the named lists.\n\n\n\nex_019.R\n\n# The package jsonlite is already loaded\n\n# Definition of the URLs\nurl_sw4 <- \"http://www.omdbapi.com/?apikey=72bc447a&i=tt0076759&r=json\"\nurl_sw3 <- \"http://www.omdbapi.com/?apikey=72bc447a&i=tt0121766&r=json\"\n\n# Import two URLs with fromJSON(): sw4 and sw3\nsw3 <- fromJSON(url_sw3)\nsw4 <- fromJSON(url_sw4)\n\n\n# Print out the Title element of both lists\nprint(sw3$Title)\nprint(sw4$Title)\n\n# Is the release year of sw4 later than sw3?\nsw4$Year > sw3$Year"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_05.html#import-sas-data-with-haven",
    "href": "_intermediate_importing_data_in_R_05.html#import-sas-data-with-haven",
    "title": "5  Importing data from statistical software packages",
    "section": "5.1 Import SAS data with haven",
    "text": "5.1 Import SAS data with haven\nhaven is an extremely easy-to-use package to import data from three software packages: SAS, STATA and SPSS. Depending on the software, you use different functions:\n\nSAS: read_sas()\nSTATA: read_dta() (or read_stata(), which are identical)\nSPSS: read_sav() or read_por(), depending on the file type.\n\nAll these functions take one key argument: the path to your local file. In fact, you can even pass a URL; haven will then automatically download the file for you before importing it.\nYou’ll be working with data on the age, gender, income, and purchase level (0 = low, 1 = high) of 36 individuals (Source: SAS). The information is stored in a SAS file, sales.sas7bdat, which is available in the dataset directory. You can also download the data here http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/sales.sas7bdat.\n\nInstructions 100 XP\n\nLoad the haven package.\nImport the data file \"sales.sas7bdat\". Call the imported data frame sales.\nDisplay the structure of sales with str(). Some columns represent categorical variables, so they should be factors.\n\n\n\nex_023.R\n\n# Load the haven package\nlibrary(haven)\n\n# Import sales.sas7bdat: sales\nsales <- read_sas(\"sales.sas7bdat\")\n\n# Display the structure of sales\nstr(sales)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_05.html#import-stata-data-with-haven",
    "href": "_intermediate_importing_data_in_R_05.html#import-stata-data-with-haven",
    "title": "5  Importing data from statistical software packages",
    "section": "5.2 Import STATA data with haven",
    "text": "5.2 Import STATA data with haven\nNext up are STATA data files; you can use read_dta() for these.\nWhen inspecting the result of the read_dta() call, you will notice that one column will be imported as a labelled vector, an R equivalent for the common data structure in other statistical environments. In order to effectively continue working on the data in R, it’s best to change this data into a standard R class. To convert a variable of the class labelled to a factor, you’ll need haven’s as_factor() function.\nIn this exercise, you will work with data on yearly import and export numbers of sugar, both in USD and in weight. The data can be found at: http://assets.datacamp.com/production/course_1478/datasets/trade.dta\n\nInstructions 100 XP\n\nImport the data file directly from the URL using read_dta(), and store it as sugar.\nPrint out the structure of sugar. The Date column has class labelled.\nConvert the values in the Date column of sugar to dates, using as.Date(as_factor(___)).\nPrint out the structure of sugar once more. Looks better now?\n\n\n\nex_024.R\n\n# haven is already loaded\nlibrary(haven)\n# Import the data from the URL: sugar\nurl <- \"http://assets.datacamp.com/production/course_1478/datasets/trade.dta\"\n\nsugar <- read_dta(url)\n\n# Structure of sugar\nstr(sugar)\n\n# Convert values in Date column to dates\nsugar$Date <- as.Date(as_factor(sugar$Date))\n\n# Structure of sugar again\nstr(sugar)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_05.html#import-spss-data-with-haven",
    "href": "_intermediate_importing_data_in_R_05.html#import-spss-data-with-haven",
    "title": "5  Importing data from statistical software packages",
    "section": "5.3 Import SPSS data with haven",
    "text": "5.3 Import SPSS data with haven\nThe haven package can also import data files from SPSS. Again, importing the data is pretty straightforward. Depending on the SPSS data file you’re working with, you’ll need either read_sav() - for .sav files - or read_por() - for .por files.\nIn this exercise, you will work with data on four of the Big Five personality traits for 434 persons (Source: University of Bath). The Big Five is a psychological concept including, originally, five dimensions of personality to classify human personality. The SPSS dataset is called person.sav and is available in your working directory.\n\nInstructions 100 XP\n\nUse read_sav() to import the SPSS data in \"person.sav\". Name the imported data frame traits.\ntraits contains several missing values, or NAs. Run summary() on it to find out how manyNAs are contained in each variable.\nPrint out a subset of those individuals that scored high on Extroversion and on Agreeableness, i.e. scoring higher than 40 on each of these two categories. You can use subset() for this.\n\n\n\nex_025.R\n\n# haven is already loaded\n\n# Import person.sav: traits\ntraits <- read_sav(\"person.sav\")\n\n# Summarize traits\nsummary(traits)\n\n# Print out a subset\n\nsubset(traits, Extroversion > 40 & Agreeableness > 40)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_05.html#factorize-round-two",
    "href": "_intermediate_importing_data_in_R_05.html#factorize-round-two",
    "title": "5  Importing data from statistical software packages",
    "section": "5.4 Factorize, round two",
    "text": "5.4 Factorize, round two\nIn the last exercise you learned how to import a data file using the command read_sav(). With SPSS data files, it can also happen that some of the variables you import have the labelled class. This is done to keep all the labelling information that was originally present in the .sav and .por files. It’s advised to coerce (or change) these variables to factors or other standard R classes.\nThe data for this exercise involves information on employees and their demographic and economic attributes (Source: QRiE). The data can be found on the following URL: http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav\n\nInstructions 100 XP\n\nImport the SPSS data straight from the URL and store the resulting data frame as work.\nDisplay the summary of the GENDER column of work. This information doesn’t give you a lot of useful information, right?\nConvert the GENDER column in work to a factor, the class to denote categorical variables in R. Use as_factor().\nOnce again display the summary of the GENDER column. This time, the printout makes much more sense.\n\n\n\nex_027.R\n\n# haven is already loaded\n\n# Import SPSS data from the URL: work\nurl <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav\"\nwork <- read_sav(url)\n\n# Display summary of work$GENDER\nsummary(work$GENDER)\n# Convert work$GENDER to a factor\nwork$GENDER <- as_factor(work$GENDER)\n# Display summary of work$GENDER again\nsummary(work$GENDER)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_05.html#import-stata-data-with-foreign-1",
    "href": "_intermediate_importing_data_in_R_05.html#import-stata-data-with-foreign-1",
    "title": "5  Importing data from statistical software packages",
    "section": "5.5 Import STATA data with foreign (1)",
    "text": "5.5 Import STATA data with foreign (1)\nThe foreign package offers a simple function to import and read STATA data: read.dta().\nIn this exercise, you will import data on the US presidential elections in the year 2000. The data in florida.dta contains the total numbers of votes for each of the four candidates as well as the total number of votes per election area in the state of Florida (Source: Florida Department of State). The file is available in the dataset directory, you can download it here http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/florida.dta\nif you want to experiment some more.\n\nInstructions 100 XP\n\nLoad the foreign package; it’s already installed on DataCamp’s servers.\nImport the data on the elections in Florida, \"florida.dta\", and name the resulting data frame florida. Use read.dta() without specifying extra arguments.\nCheck out the last 6 observations of florida with tail().\n\n\n\nex_028.R\n\n# Load the foreign package\nlibrary(foreign)\n\n# Import florida.dta and name the resulting data frame florida\n\nflorida <- read.dta(\"florida.dta\")\n# Check tail() of florida\ntail(florida)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_05.html#import-stata-data-with-foreign-2",
    "href": "_intermediate_importing_data_in_R_05.html#import-stata-data-with-foreign-2",
    "title": "5  Importing data from statistical software packages",
    "section": "5.6 Import STATA data with foreign (2)",
    "text": "5.6 Import STATA data with foreign (2)\nData can be very diverse, going from character vectors to categorical variables, dates and more. It’s in these cases that the additional arguments of read.dta() will come in handy.\nThe arguments you will use most often are convert.dates, convert.factors, missing.type and convert.underscore. Their meaning is pretty straightforward, as Filip explained in the video. It’s all about correctly converting STATA data to standard R data structures. Type ?read.dta to find out about about the default values.\nThe dataset for this exercise contains socio-economic measures and access to education for different individuals (Source: World Bank). This data is available as edequality.dta, which is located in the worldbank folder in your working directory.\n\nInstructions 100 XP\n\n\nex_029.R\n\n# foreign is already loaded\n\n# Specify the file path using file.path(): path\npath <- file.path(\"worldbank\", \"edequality.dta\")\n\n# Create and print structure of edu_equal_1\nedu_equal_1 <- read.dta(path)\nstr(edu_equal_1)\n\n\n# Create and print structure of edu_equal_2\nedu_equal_2 <- \n    read.dta(\n        path,\n        convert.factors = FALSE\n    )\nstr(edu_equal_2)\n\n\n# Create and print structure of edu_equal_3\n\nedu_equal_3 <- \n    read.dta(\n        path,\n        convert.underscore = TRUE\n    )\nstr(edu_equal_3)\n\n\nnrow(\n    subset(\n        edu_equal_1,\n        edu_equal_1$ethnicity_head == \"Bulgarian\",\n        income > 1000\n    )\n)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_05.html#import-spss-data-with-foreign-1",
    "href": "_intermediate_importing_data_in_R_05.html#import-spss-data-with-foreign-1",
    "title": "5  Importing data from statistical software packages",
    "section": "5.7 Import SPSS data with foreign (1)",
    "text": "5.7 Import SPSS data with foreign (1)\nAll great things come in pairs. Where foreign provided read.dta() to read Stata data, there’s also read.spss() to read SPSS data files. To get a data frame, make sure to set to.data.frame = TRUE inside read.spss().\nIn this exercise, you’ll be working with socio-economic variables from different countries (Source: Quantative Data Analysis in Education). The SPSS data is in a file called international.sav, which is in your working directory. You can also download it here if you want to play around with it some more.\n\nInstructions 100 XP\n\n\nex_030.R\n\n# foreign is already loaded\n\n# Import international.sav as a data frame: demo\n\ndemo <- read.spss(\n    \"international.sav\", to.data.frame=TRUE)\n# Create boxplot of gdp variable of demo\nboxplot(demo$gdp)"
  },
  {
    "objectID": "_intermediate_importing_data_in_R_05.html#import-spss-data-with-foreign-2",
    "href": "_intermediate_importing_data_in_R_05.html#import-spss-data-with-foreign-2",
    "title": "5  Importing data from statistical software packages",
    "section": "5.8 Import SPSS data with foreign (2)",
    "text": "5.8 Import SPSS data with foreign (2)\nIn the previous exercise, you used the to.data.frame argument inside read.spss(). There are many other ways in which to customize the way your SPSS data is imported.\nIn this exercise you will experiment with another argument, use.value.labels. It specifies whether variables with value labels should be converted into R factors with levels that are named accordingly. The argument is TRUE by default which means that so called labelled variables inside SPSS are converted to factors inside R.\nYou’ll again be working with the international.sav data, which is available in your current working directory.\n\nInstructions 100 XP\n\nImport the data file \"international.sav\" as a data frame, demo_1.\nPrint the first few rows of demo_1 using the head() function.\nImport the data file \"international.sav\" as a data frame, demo_2, but this time in a way such that variables with value labels are not converted to R factors.\nAgain, print the first few rows of demo_2. Can you tell the difference between the two data frames?\n\n\n\nex_031.R\n\n# foreign is already loaded\n\n# Import international.sav as demo_1\ndemo_1 <- read.spss(\"international.sav\")\n# Print out the head of demo_1\nhead(demo)\n\n# Import international.sav as demo_2\ndemo_2 <- read.spss(\n    \"international.sav\",\n    convert.factors = FALSE\n    )\n\n# Print out the head of demo_2\nhead(demo_2)"
  }
]