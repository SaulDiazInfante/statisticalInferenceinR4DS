[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Fundamentals With R",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "_introduction_to_statistics_in_r_01.html#mean-and-median",
    "href": "_introduction_to_statistics_in_r_01.html#mean-and-median",
    "title": "1  Summary Statistics",
    "section": "1.1 Mean and Median",
    "text": "1.1 Mean and Median\nIn this chapter, you’ll be working with the 2018 Food Carbon Footprint Index from nu3. The food_consumption dataset contains information about the kilograms of food consumed per person per year in each country in each food category (consumption) as well as information about the carbon footprint of that food category (co2_emissions) measured in kilograms of carbon dioxide, or CO\\(_2\\), per person per year in each country.\nIn this exercise, you’ll compute measures of center to compare food consumption in the US and Belgium using your dplyr skills.\ndplyr is loaded for you and food_consumption is available. ### Instructions 100 XP {.unnumbered}\n\nCreate two data frames: one that holds the rows of food_consumption for “Belgium” and the another that holds rows for “USA”. Call these belgium_consumption and usa_consumption.\nCalculate the mean and median of kilograms of food consumed per person per year for both countries.\nFilter food_consumption for rows with data about Belgium and the USA.\nGroup the filtered data by country.\nCalculate the mean and median of the kilograms of food consumed per person per year in each country. Call these columns mean_consumption and median_consumption.\n\n\n\nex_001.R\n\n# Filter for Belgium\nbelgium_consumption <- food_consumption %>%\n  filter(country==\"Belgium\")\n\n# Filter for USA\nusa_consumption <- food_consumption %>%\n  filter(country==\"USA\")\n\n# Calculate mean and median consumption in Belgium\nmean(belgium_consumption$consumption)\nmedian(belgium_consumption$consumption)\n\n# Calculate mean and median consumption in USA\nmean(usa_consumption$consumption)\nmedian(usa_consumption$consumption)\n\nfood_consumption %>%\n  # Filter for Belgium and USA\n  filter(country %in% c(\"Belgium\", \"USA\")) %>%\n  # Group by country\n  group_by(country) %>%\n  # Get mean_consumption and median_consumption\n  sumarize(mean_consumption = mean(consumption),\n      median_consumption = median(consumption))"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_01.html#mean-vs.-median",
    "href": "_introduction_to_statistics_in_r_01.html#mean-vs.-median",
    "title": "1  Summary Statistics",
    "section": "1.2 Mean vs. Median",
    "text": "1.2 Mean vs. Median\nIn the video, you learned that the mean is the sum of all the data points divided by the total number of data points, and the median is the middle value of the dataset where 50% of the data is less than the median, and 50% of the data is greater than the median. In this exercise, you’ll compare these two measures of center.\ndplyr and ggplot2 are loaded and food_consumption is available.\n\nInstructions 100 XP\n\nFilter food_consumption to get the rows where food_category is “rice”.\nCreate a histogram using ggplot2 of co2_emission for rice.\nFilter food_consumption to get the rows where food_category is “rice”.\nSummarize the data to get the mean and median of co2_emission, calling them mean_co2 and median_co2.\n\n\n\nex_002.R\n\nfood_consumption %>%\n  # Filter for rice food category\n  filter(food_category == \"rice\") %>%\n  # Create histogram of co2_emission\n  ggplot(aes(co2_emission)) +\n    geom_histogram()\n\nfood_consumption %>%\n  # Filter for rice food category\n  filter(food_category==\"rice\") %>% \n  # Get mean_co2 and median_co2\n  summarize(\n    mean_co2 = mean(co2_emission),\n    median_co2 = median(co2_emission)\n  )"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_01.html#quartiles-quantiles-and-quintiles",
    "href": "_introduction_to_statistics_in_r_01.html#quartiles-quantiles-and-quintiles",
    "title": "1  Summary Statistics",
    "section": "1.3 Quartiles, quantiles, and quintiles",
    "text": "1.3 Quartiles, quantiles, and quintiles\nQuantiles are a great way of summarizing numerical data since they can be used to measure center and spread, as well as to get a sense of where a data point stands in relation to the rest of the dataset. For example, you might want to give a discount to the 10% most active users on a website.\nIn this exercise, you’ll calculate quartiles, quintiles, and deciles, which split up a dataset into 4, 5, and 10 pieces, respectively.\nThe dplyr package is loaded and food_consumption is available.\n\nInstructions 100 XP\n\nCalculate the quartiles of the co2_emission column of food_consumption.\nCalculate the six quantiles that split up the data into 5 pieces (quintiles) of the co2_emission column of food_consumption.\nCalculate the eleven quantiles of co2_emission that split up the data into ten pieces (deciles).\n\n\n\nex_003.R\n\nquantile(food_consumption$co2_emission)\n# Calculate the quintiles of co2_emission\nquantile(\n    food_consumption$co2_emission,\n    probs = c(0.0, 0.2, 0.4, 0.6, 0.8, 1))\n# Calculate the deciles of co2_emission\nquantile(\n    food_consumption$co2_emission,\n    probs = c(0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1))"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_01.html#variance-and-standard-deviation",
    "href": "_introduction_to_statistics_in_r_01.html#variance-and-standard-deviation",
    "title": "1  Summary Statistics",
    "section": "1.4 Variance and standard deviation",
    "text": "1.4 Variance and standard deviation\nVariance and standard deviation are two of the most common ways to measure the spread of a variable, and you’ll practice calculating these in this exercise. Spread is important since it can help inform expectations. For example, if a salesperson sells a mean of 20 products a day, but has a standard deviation of 10 products, there will probably be days where they sell 40 products, but also days where they only sell one or two. Information like this is important, especially when making predictions.\nBoth dplyr and ggplot2 are loaded, and food_consumption is available.\n\nInstructions 100 XP\n\nCalculate the variance and standard deviation of co2_emission for each food_category by grouping by and summarizing variance as var_co2 and standard deviation as sd_co2.\nCreate a histogram of co2_emission for each food_category using facet_wrap().\n\n\n\nex_005.R\n\n# Calculate variance and sd of co2_emission for each food_category\nfood_consumption %>% \n  group_by(food_category) %>% \n  summarize(var_co2 = var(co2_emission),\n     sd_co2 = sd(co2_emission))\n\n# Plot food_consumption with co2_emission on x-axis\nggplot(data = food_consumption, aes(co2_emission)) +\n  # Create a histogram\n  geom_histogram() +\n  # Create a separate sub-graph for each food_category\n  facet_wrap(~ food_category)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_01.html#finding-outliers-using-iqr",
    "href": "_introduction_to_statistics_in_r_01.html#finding-outliers-using-iqr",
    "title": "1  Summary Statistics",
    "section": "1.5 Finding outliers using IQR",
    "text": "1.5 Finding outliers using IQR\nIn this exercise, you’ll calculate IQR and use it to find some outliers. Both dplyr and ggplot2 are loaded and food_consumption is available. \n\nInstructions 100 XP\n\nCalculate the total co2_emission per country by grouping by country and taking the sum of co2_emission. Call the sum total_emission and store the resulting data frame as emissions_by_country.\nCompute the first and third quartiles of total_emission and store these as q1 and q3.\nCalculate the interquartile range of total_emission and store it as iqr.\nCalculate the lower and upper cutoffs for outliers of total_emission, and store these as lower and upper.\nUse filter() to get countries with a total_emission greater than the upper cutoff or a total_emission less than the lower cutoff.\n\n\n\nex_006.R\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country <- food_consumption %>%\n  group_by(country) %>%\n  summarize(total_emission = sum(co2_emission))\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country <- food_consumption %>%\n  group_by(country) %>%\n  summarize(total_emission = sum(co2_emission))\n\n# Compute the first and third quartiles and IQR of total_emission\nq1 <- quantile(emissions_by_country$total_emission, c(0.25))\nq3 <- quantile(emissions_by_country$total_emission, c(0.75))\niqr <- q3 - q1\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country <- food_consumption %>%\n  group_by(country) %>%\n  summarize(total_emission = sum(co2_emission))\n\n# Compute the first and third quartiles and IQR of total_emission\nq1 <- quantile(emissions_by_country$total_emission, 0.25)\nq3 <- quantile(emissions_by_country$total_emission, 0.75)\niqr <- q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower <- q1 - 1.5 * iqr\nupper <- q3 + 1.5 * iqr\n\n# Filter emissions_by_country to find outliers\nemissions_by_country %>%\n  filter(total_emission > upper | total_emission < lower)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#calculating-probabilities",
    "href": "_introduction_to_statistics_in_r_02.html#calculating-probabilities",
    "title": "2  Random Numbers and Probability",
    "section": "2.1 Calculating probabilities",
    "text": "2.1 Calculating probabilities\nYou’re in charge of the sales team, and it’s time for performance reviews, starting with Amir. As part of the review, you want to randomly select a few of the deals that he’s worked on over the past year so that you can look at them more deeply. Before you start selecting deals, you’ll first figure out what the chances are of selecting certain deals.\nRecall that the probability of an event can be calculated by \\[\nP(event) =\n    \\frac{\n        \\text{ways\\# event can appen}\n    }{\n        \\text{total \\# of possible outcomes}\n    }\n\\] dplyr is loaded and amir_deals is available. ### Instuctions 100 xp {.unnumbered}\n\nCount the number of deals Amir worked on for each product type.\nCreate a new column called prob by dividing n by the total number of deals Amir worked on.\n\n\n\nex_007.R\n\n# Count the deals for each product\namir_deals %>%\n  count(product)\n# Calculate probability of picking a deal with each product\namir_deals %>%\n  count(product) %>%\n  mutate(prob = n / sum(n))"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#sampling-deals",
    "href": "_introduction_to_statistics_in_r_02.html#sampling-deals",
    "title": "2  Random Numbers and Probability",
    "section": "2.2 Sampling deals",
    "text": "2.2 Sampling deals\nIn the previous exercise, you counted the deals Amir worked on. Now it’s time to randomly pick five deals so that you can reach out to each customer and ask if they were satisfied with the service they received. You’ll try doing this both with and without replacement.\nAdditionally, you want to make sure this is done randomly and that it can be reproduced in case you get asked how you chose the deals, so you’ll need to set the random seed before sampling from the deals.\ndplyr is loaded and amir_deals is available.\n\nInstuctions 100 xp\n\nSet the random seed to 31.\nTake a sample of 5 deals without replacement.\n\n\n\nex_008.R\n\n# Set random seed to 31\nset.seed(31)\n\n# Sample 5 deals without replacement\namir_deals %>%\n  sample_n(5)\n\n# Set random seed to 31\nset.seed(31)\n\n# Sample 5 deals with replacement\namir_deals %>%\n  sample_n(5, replace = TRUE)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#creatiing-a-probability-distribution",
    "href": "_introduction_to_statistics_in_r_02.html#creatiing-a-probability-distribution",
    "title": "2  Random Numbers and Probability",
    "section": "2.3 Creatiing a probability distribution",
    "text": "2.3 Creatiing a probability distribution\nA new restaurant opened a few months ago, and the restaurant’s management wants to optimize its seating space based on the size of the groups that come most often. On one night, there are 10 groups of people waiting to be seated at the restaurant, but instead of being called in the order they arrived, they will be called randomly. In this exercise, you’ll investigate the probability of groups of different sizes getting picked first. Data on each of the ten groups is contained in the restaurant_groups data frame.\nRemember that expected value can be calculated by multiplying each possible outcome with its corresponding probability and taking the sum. The restaurant_groups data is available and dplyr and ggplot2 are loaded.\n\nInstuctions 100 xp\n\nCreate a histogram of group_size\nCount the number of each group_size in restaurant_groups, then add a column called probability that contains the probability of randomly selecting a group of each size. Store this in a new data frame called size_distribution.\nCalculate the expected value of the size_distribution, which represents the expected group size.\nCalculate the probability of randomly picking a group of 4 or more people by filtering and summarizing.\n\n\n\nex_009.R\n\n# Create a histogram of group_size\nggplot(data = restaurant_groups, aes(x=group_size)) +\n  geom_histogram(bins=5)\n\n# Create probability distribution\nsize_distribution <- restaurant_groups %>%\n  # Count number of each group size\n  count(group_size) %>%\n  # Calculate probability\n  mutate(probability = n / sum(n))\n\nsize_distribution\nexpected_val <- \n  sum(\n    size_distribution$group_size *\n    size_distribution$probability)\nexpected_val\n\n# Create probability distribution\nsize_distribution <- restaurant_groups %>%\n  count(group_size) %>%\n  mutate(probability = n / sum(n))\n\n# Calculate probability of picking group of 4 or more\nsize_distribution %>%\n  # Filter for groups of 4 or larger\n  filter(group_size >= 4) %>%\n  # Calculate prob_4_or_more by taking sum of probabilities\n  summarize(\n    prob_4_or_more = \n    sum(probability)\n  )"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#section",
    "href": "_introduction_to_statistics_in_r_02.html#section",
    "title": "2  Random Numbers and Probability",
    "section": "2.9 ",
    "text": "2.9 \n\nInstuctions 100 xp\n\n\nex_.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#section-1",
    "href": "_introduction_to_statistics_in_r_02.html#section-1",
    "title": "2  Random Numbers and Probability",
    "section": "2.5 ",
    "text": "2.5 \n\nInstuctions 100 xp\n\n\nex_010.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#section-2",
    "href": "_introduction_to_statistics_in_r_02.html#section-2",
    "title": "2  Random Numbers and Probability",
    "section": "2.6 ",
    "text": "2.6 \n\nInstuctions 100 xp\n\n\nex_.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#section-3",
    "href": "_introduction_to_statistics_in_r_02.html#section-3",
    "title": "2  Random Numbers and Probability",
    "section": "2.7 ",
    "text": "2.7 \n\nInstuctions 100 xp\n\n\nex_.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#section-4",
    "href": "_introduction_to_statistics_in_r_02.html#section-4",
    "title": "2  Random Numbers and Probability",
    "section": "2.8 ",
    "text": "2.8 \n\nInstuctions 100 xp\n\n\nex_.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#section-5",
    "href": "_introduction_to_statistics_in_r_02.html#section-5",
    "title": "2  Random Numbers and Probability",
    "section": "2.9 ",
    "text": "2.9 \n\nInstuctions 100 xp\n\n\nex_.R"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#data-back-ups",
    "href": "_introduction_to_statistics_in_r_02.html#data-back-ups",
    "title": "2  Random Numbers and Probability",
    "section": "2.4 Data back-ups",
    "text": "2.4 Data back-ups\nThe sales software used at your company is set to automatically back itself up, but no one knows exactly what time the back-ups happen. It is known, however, that back-ups happen exactly every 30 minutes. Amir comes back from sales meetings at random times to update the data on the client he just met with. He wants to know how long he’ll have to wait for his newly-entered data to get backed up. Use your new knowledge of continuous uniform distributions to model this situation and answer Amir’s questions.\n\nInstuctions 100 xp\n\n\nex_010.R\n\n# Min and max wait times for back-up that happens every 30 min\nmin <- 0\nmax <- 30\n# Min and max wait times for back-up that happens every 30 min\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 <- 5/30\nprob_less_than_5\n\n# Min and max wait times for back-up that happens every 30 min\nmin <- 0\nmax <- 30\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 <- 5/30\nprob_less_than_5\n\n# Min and max wait times for back-up that happens every 30 min\nmin <- 0\nmax <- 30\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 <- 5/30\nprob_less_than_5"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#simulating-wait-times",
    "href": "_introduction_to_statistics_in_r_02.html#simulating-wait-times",
    "title": "2  Random Numbers and Probability",
    "section": "2.5 Simulating wait times",
    "text": "2.5 Simulating wait times\nTo give Amir a better idea of how long he’ll have to wait, you’ll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Recall from the last exercise that his minimum wait time is 0 minutes and his maximum wait time is 30 minutes.\nA data frame called is available and dplyr and ggplot2 are loaded.\n\nInstuctions 100 xp\n\nSet the random seed to 334.\nGenerate 1000 wait times from the continuous uniform distribution that models Amir’s wait time. Add this as a new column called time in the wait_times data frame.\nCreate a histogram of the simulated wait times with 30 bins.\n\n\n\nex_011.R\n\n# Set random seed to 334\nset.seed(334)\n# Generate 1000 wait times between 0 and 30 mins, save in time column\nwait_times %>%\n  mutate(time = runif(1000, min = 0, max = 30)) %>%\n  # Create a histogram of simulated times\n  ggplot(aes(x = time)) +\n    geom_histogram(n_bins=30)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#simulating-sales-deals",
    "href": "_introduction_to_statistics_in_r_02.html#simulating-sales-deals",
    "title": "2  Random Numbers and Probability",
    "section": "2.6 Simulating sales deals",
    "text": "2.6 Simulating sales deals\nAssume that Amir usually works on 3 deals per week, and overall, he wins 30% of deals he works on. Each deal has a binary outcome: it’s either lost, or won, so you can model his sales deals with a binomial distribution. In this exercise, you’ll help Amir simulate a year’s worth of his deals so he can better understand his performance.\n\nInstuctions 100 xp\n\nSet the random seed to 10 and simulate a single deal.\nSimulate a typical week of Amir’s deals, or one week of 3 deals.\nSimulate a year’s worth of Amir’s deals, or 52 weeks of 3 deals each, and store in deals. Calculate the mean number of deals he won per week\n\n\n\nex_012.R\n\n# Set random seed to 10\nset.seed(10)\n# Simulate a single deal\nrbinom(1, 1, 0.3)\nrbinom(1, 3, 0.3)\n# Set random seed to 10\n# Simulate 52 weeks of 3 deals\ndeals <- rbinom(52, 3, 0.3)\n\n# Calculate mean deals won per week\nmean(deals)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#calculating-binomial-probabilities",
    "href": "_introduction_to_statistics_in_r_02.html#calculating-binomial-probabilities",
    "title": "2  Random Numbers and Probability",
    "section": "2.7 Calculating binomial probabilities",
    "text": "2.7 Calculating binomial probabilities\nJust as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you’ll calculate what the chances are of him closing different numbers of deals using the binomial distribution.\n\nInstuctions 100 xp\n\nWhat’s the probability that Amir closes all 3 deals in a week?\n\n\n\nex_013.R\n\n# Probability of closing 3 out of 3 deals\ndbinom(3, 3, 0.3)\n\n# Probability of closing <= 1 deal out of 3 deals\npbinom(1, 3, 0.3)\n\n# Probability of closing > 1 deal out of 3 deals\n1 - pbinom(1, 3, 0.3,m lower.tail=FALSE)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_02.html#how-many-sales-will-be-won",
    "href": "_introduction_to_statistics_in_r_02.html#how-many-sales-will-be-won",
    "title": "2  Random Numbers and Probability",
    "section": "2.8 How many sales will be won?",
    "text": "2.8 How many sales will be won?\nNow Amir wants to know how many deals he can expect to close each week if his win rate changes. Luckily, you can use your binomial distribution knowledge to help him calculate the expected value in different situations. Recall from the video that the expected value of a binomial distribution can be calculated by \\(n \\times p\\).\n\nInstuctions 100 xp\n\nCalculate the expected number of sales out of the 3 he works on that Amir will win each week if he maintains his 30% win rate.\nCalculate the expected number of sales out of the 3 he works on that he’ll win if his win rate drops to 25%.\nCalculate the expected number of sales out of the 3 he works on that he’ll win if his win rate rises to 35%.\n\n\n\nex_014.R\n\n# Expected number won with 30% win rate\nwon_30pct <- 3 * 0.3\nwon_30pct \n\n# Expected number won with 25% win rate\nwon_25pct <- 3 * 0.25\nwon_25pct\n\n# Expected number won with 35% win rate\nwon_35pct <- 3 * 0.35\nwon_35pct"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#section",
    "href": "_introduction_to_statistics_in_r_03.html#section",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.4 ",
    "text": "3.4 \n\n3.4.1 Instructions 100 XP\n\n\nex_019.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#section-1",
    "href": "_introduction_to_statistics_in_r_03.html#section-1",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.5 ",
    "text": "3.5 \n\n3.5.1 Instructions 100 XP\n\n\nex_020.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#section-2",
    "href": "_introduction_to_statistics_in_r_03.html#section-2",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.6 ",
    "text": "3.6 \n\n3.6.1 Instructions 100 XP\n\n\nex_21.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#section-3",
    "href": "_introduction_to_statistics_in_r_03.html#section-3",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.7 ",
    "text": "3.7 \n\n3.7.1 Instructions 100 XP\n\n\nex_022.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#section-4",
    "href": "_introduction_to_statistics_in_r_03.html#section-4",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.8 ",
    "text": "3.8 \n\n3.8.1 Instructions 100 XP\n\n\nex_023.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#probabilities-from-the-normal-distribution",
    "href": "_introduction_to_statistics_in_r_03.html#probabilities-from-the-normal-distribution",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.2 Probabilities from the normal distribution",
    "text": "3.2 Probabilities from the normal distribution\nSince each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals and follow a normal distribution with a mean of 5000 dollars and a standard deviation of 2000 dollars. As part of his performance metrics, you want to calculate the probability of Amir closing a deal worth various amounts.\n\n3.2.1 Instructions 100 XP\n\nWhat’s the probability of Amir closing a deal worth less than $7500?\n\n\n\nex_017.R\n\n# Probability of deal < 7500\npnorm(7500, 5000,  2000)\n# Probability of deal > 1000\n1 - pnorm(1000, 5000, 2000)\n# Probability of deal between 3000 and 7000\npnorm(7000, 5000, 2000) - pnorm(3000, 5000, 2000)\n# Calculate amount that 75% of deals will be more than\nqnorm(0.75, 5000, 2000, lower.tail = FALSE)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#distribution-of-amirs-sales",
    "href": "_introduction_to_statistics_in_r_03.html#distribution-of-amirs-sales",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.1 Distribution of Amir’s sales",
    "text": "3.1 Distribution of Amir’s sales\nSince each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals As part of Amir’s performance review, you want to be able to estimate the probability of him selling different amounts, but before you can do this, you’ll need to determine what kind of distribution the amount variable follows.\nBoth dplyr and ggplot2 are loaded and amir_deals is available.\n\n3.1.1 Instructions 100 XP\n\nCreate a histogram with 10 bins to visualize the distribution of the amount.\n\n\n\nex_016.R\n\n# Histogram of amount with 10 bins\namir_deals %>%\n    ggplot(aes(x = amount)) +\n        geom_histogram(bins=10)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#section-5",
    "href": "_introduction_to_statistics_in_r_03.html#section-5",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.9 ",
    "text": "3.9 \n\n3.9.1 Instructions 100 XP\n\n\nex_024.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#section-6",
    "href": "_introduction_to_statistics_in_r_03.html#section-6",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.10 ",
    "text": "3.10"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html",
    "href": "_introduction_to_statistics_in_r_03.html",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "",
    "text": "4 Calculate new standard deviation\nnew_sd <- 2000 * (1.3)\nnew_sales <- new_sales %>% mutate(amount = rnorm(36, new_mean, new_sd))\nnew_sales %>% ggplot(aes(x = amount)) + geom_histogram(bins = 10)\nnew_sd <- 2000 * (1.3)\nnew_sales <- new_sales %>% mutate(amount = rnorm(36, new_mean, new_sd))\nnew_sales %>% ggplot(aes(x = amount)) + geom_histogram(bins = 10)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#simulating-sales-under-new-market-conditions",
    "href": "_introduction_to_statistics_in_r_03.html#simulating-sales-under-new-market-conditions",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.3 Simulating sales under new market conditions",
    "text": "3.3 Simulating sales under new market conditions\nThe company’s financial analyst is predicting that next quarter, the worth of each sale will increase by 20% and the volatility, or standard deviation, of each sale’s worth will increase by 30%. To see what Amir’s sales might look like next quarter under these new market conditions, you’ll simulate new sales amounts using the normal distribution and store these in the new_sales data frame, which has already been created for you.\n\n3.3.1 Instructions 100 XP\n\nCurrently, Amir’s average sale amount is $5000.\nCalculate what his new average amount will be if it increases by 20% and store this in new_mean.\nAmir’s current standard deviation is $2000. Calculate what his new standard deviation will be if it increases by 30% and store this in new_sd.\nAdd a new column called amount to the data frame new_sales, which contains 36 simulated amounts from a normal distribution with a mean of new_mean and a standard deviation of new_sd.\nPlot the distribution of the new_sales amounts using a histogram with 10 bins.\n\n\n\nex_018.R\n\n# Calculate new average amount\nnew_mean <- 5000 * (1.2)\n\n# Calculate new standard deviation\nnew_sd <- 2000 * (1.3)\n\n# Simulate 36 sales\nnew_sales <- new_sales %>% \n  mutate(amount = rnorm(36, new_mean, new_sd))\n\n# Create histogram with 10 bins\nnew_sales %>%\n  ggplot(aes(x = amount)) + \n    geom_histogram(bins = 10)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_04.html#relationships-between-variables",
    "href": "_introduction_to_statistics_in_r_04.html#relationships-between-variables",
    "title": "4  Correlation and Experimental Design",
    "section": "4.1 Relationships between variables",
    "text": "4.1 Relationships between variables\nIn this chapter, you’ll be working with a dataset world_happiness containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.\nIn this exercise, you’ll examine the relationship between a country’s life expectancy (life_exp) and happiness score (happiness_score) both visually and quantitatively. Both dplyr and ggplot2 are loaded and world_happiness is available.\n\n4.1.1 Instructions 100 XP\n\nCreate a scatterplot of happiness_score vs. life_exp using ggplot2\nAdd a linear trendline to the scatterplot, setting se to FALSE.\n\n\n\nex_023.R\n\n# Create a scatterplot of happiness_score vs. life_exp\nworld_happiness %>%\n    ggplot(aes(x = life_exp, y = happiness_score )) + \n        geom_point()\n# Add a linear trendline to scatterplot\nggplot(world_happiness, aes(life_exp, happiness_score)) +\n  geom_point() +\n  geom_smooth(method =\"lm\", se =FALSE)\ncor(\n  world_happiness$life_exp,\n  world_happiness$happiness_score\n)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_04.html#what-cant-correlation-measure",
    "href": "_introduction_to_statistics_in_r_04.html#what-cant-correlation-measure",
    "title": "4  Correlation and Experimental Design",
    "section": "4.2 What can’t correlation measure?",
    "text": "4.2 What can’t correlation measure?\nWhile the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it’s far from perfect. In this exercise, you’ll explore one of the caveats of the correlation coefficient by examining the relationship between a country’s GDP per capita (gdp_per_cap) and happiness score.\nBoth dplyr and ggplot2 are loaded and world_happiness is available.\n\n4.2.1 Instructions 100 XP\nCreate a scatterplot showing the relationship between gdp_per_cap (on the x-axis) and life_exp (on the y-axis).\n\n\nex_024.R\n\n# Scatterplot of gdp_per_cap and life_exp\nworld_happiness %>%\n    ggplot(aes(x = gdp_per_cap, y = life_exp)) +\n    geom_point()\ncor(world_happiness$gdp_per_cap,\n  world_happiness$life_exp\n)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_04.html#transforming-variables",
    "href": "_introduction_to_statistics_in_r_04.html#transforming-variables",
    "title": "4  Correlation and Experimental Design",
    "section": "4.3 Transforming variables",
    "text": "4.3 Transforming variables\nWhen variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. In this exercise, you’ll perform a transformation yourself.\nBoth dplyr and ggplot2 are loaded and world_happiness is available. ### Instructions 100 XP\n\n\nex_025.R\n\n# Scatterplot of happiness_score vs. gdp_per_cap\nworld_happiness %>%\n    ggplot(aes(x = gdp_per_cap, y = happiness_score)) + \n    geom_point()\n\n# Calculate correlation\ncor(world_happiness$happiness_score, \nworld_happiness$gdp_per_cap)\n# Create log_gdp_per_cap column\nworld_happiness <- world_happiness %>%\n  mutate(log_gdp_per_cap = log(gdp_per_cap))\n\n# Scatterplot of happiness_score vs. log_gdp_per_cap\nggplot(world_happiness,\n  aes(\n    x = log_gdp_per_cap,\n    y = happiness_score)\n  ) +\n  geom_point()\n\n# Calculate correlation\ncor(\n  world_happiness$log_gdp_per_cap,\n  world_happiness$happiness_score\n)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_04.html#does-sugar-improve-happiness",
    "href": "_introduction_to_statistics_in_r_04.html#does-sugar-improve-happiness",
    "title": "4  Correlation and Experimental Design",
    "section": "4.4 Does sugar improve happiness?",
    "text": "4.4 Does sugar improve happiness?\nA new column has been added to world_happiness called grams_sugar_per_day, which contains the average amount of sugar eaten per person per day in each country. In this exercise, you’ll examine the effect of a country’s average sugar consumption on its happiness score.\nBoth dplyr and ggplot2 are loaded and world_happiness is available. ### Instructions 100 XP\n\n\nex_026.R\n\n# Scatterplot of grams_sugar_per_day and happiness_score\nworld_happiness %>%\n    ggplot(\n        aes(\n            x = grams_sugar_per_day,\n            y = happiness_score\n            \n        )\n    )+ \n        geom_point()\n\n# Correlation between grams_sugar_per_day and happiness_score\ncor(\n    world_happiness$happiness_score, \n    world_happiness$grams_sugar_per_day\n)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_04.html#section",
    "href": "_introduction_to_statistics_in_r_04.html#section",
    "title": "4  Correlation and Experimental Design",
    "section": "4.5 ",
    "text": "4.5 \n\n4.5.1 Instructions 100 XP\n\n\nex_027.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_04.html#section-1",
    "href": "_introduction_to_statistics_in_r_04.html#section-1",
    "title": "4  Correlation and Experimental Design",
    "section": "4.6 ",
    "text": "4.6 \n\n4.6.1 Instructions 100 XP\n\n\nex_028.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_04.html#section-2",
    "href": "_introduction_to_statistics_in_r_04.html#section-2",
    "title": "4  Correlation and Experimental Design",
    "section": "4.7 ",
    "text": "4.7 \n\n4.7.1 Instructions 100 XP\n\n\nex_029.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_04.html#section-3",
    "href": "_introduction_to_statistics_in_r_04.html#section-3",
    "title": "4  Correlation and Experimental Design",
    "section": "4.8 ",
    "text": "4.8 \n\n4.8.1 Instructions 100 XP\n\n\nex_030.R"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#the-clt-in-action",
    "href": "_introduction_to_statistics_in_r_03.html#the-clt-in-action",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.4 The CLT in action",
    "text": "3.4 The CLT in action\nThe central limit theorem states that a sampling distribution of a sample statistic approaches the normal distribution as you take more samples, no matter the original distribution being sampled from.\nIn this exercise, you’ll focus on the sample mean and see the central limit theorem in action while examining the num_users column of amir_deals more closely, which contains the number of people who intend to use the product Amir is selling.\nBoth dplyr and ggplot2 are loaded and amir_deals is available.\n\n3.4.1 Instructions 100 XP\n\nCreate a histogram of the num_users column of amir_deals. Use 10 bins.\nSet the seed to 104.\nTake a sample of size 20 with replacement from the num_users column of amir_deals, and take the mean.\n\n\n\nex_019.R\n\n# Create a histogram of num_users\namir_deals %>%\n    ggplot(aes(x = num_users)) +\n        geom_histogram(bins = 10) \n\n# Set seed to 104\nset.seed(104)\n\n# Sample 20 num_users with replacement from amir_deals\nsample(amir_deals$num_users, 20, replace = TRUE) %>%\n  # Take mean\n  mean()\n\n# Set seed to 104\nset.seed(104)\n\n# Sample 20 num_users from amir_deals and take mean\nsample(amir_deals$num_users, size = 20, replace = TRUE) %>%\n  mean()\n\n# Repeat the above 100 times\nsample_means <- \n  replicate(\n    100,\n    sample(\n      amir_deals$num_users,\n      size = 20,\n      replace = TRUE\n    ) %>% mean()\n  )\n\n# Set seed to 104\nset.seed(104)\n\n# Sample 20 num_users from amir_deals and take mean\nsample(amir_deals$num_users, size = 20, replace = TRUE) %>%\n  mean()\n\n# Repeat the above 100 times\nsample_means <- \n  replicate(\n    100,\n    sample(\n      amir_deals$num_users,\n      size = 20,\n      replace = TRUE\n    ) %>% \n      mean()\n  )\n\n# Create data frame for plotting\nsamples <- data.frame(mean = sample_means)\n\n# Histogram of sample means\nsamples %>%\n  ggplot(aes(x = mean)) +\n    geom_histogram(bins = 10)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#the-mean-of-means",
    "href": "_introduction_to_statistics_in_r_03.html#the-mean-of-means",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.5 The mean of means",
    "text": "3.5 The mean of means\nYou want to know what the average number of users (num_users) is per deal, but you want to know this number for the entire company so that you can see if Amir’s deals have more or fewer users than the company’s average deal. The problem is that over the past year, the company has worked on more than ten thousand deals, so it’s not realistic to compile all the data. Instead, you’ll estimate the mean by taking several random samples of deals, since this is much easier than collecting data from everyone in the company.\nThe user data for all the company’s deals is available in all_deals.\n\n3.5.1 Instructions 100 XP\n\nSet the random seed to 321.\nTake 30 samples of size 20 from all_deals$num_users and take the mean of each sample. Store the sample means in sample_means.\nTake the mean of sample_means.\nTake the mean of the num_users column of amir_deals.\n\n\n\nex_020.R\n\n# Set seed to 321\nset.seed(321)\n\n# Take 30 samples of 20 values of num_users, take mean of each sample\nsample_means <- \n    replicate(\n        30,\n        sample(\n            all_deals$num_users,\n            20\n        ) %>% \n            mean()\n    )\n\n# Calculate mean of sample_means\nmean(sample_means)\n\n# Calculate mean of num_users in amir_deals\nmean(amir_deals$num_users)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#tracking-lead-responses",
    "href": "_introduction_to_statistics_in_r_03.html#tracking-lead-responses",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.6 Tracking lead responses",
    "text": "3.6 Tracking lead responses\nYour company uses sales software to keep track of new sales leads. It organizes them into a queue so that anyone can follow up on one when they have a bit of free time. Since the number of lead responses is a countable outcome over a period of time, this scenario corresponds to a Poisson distribution. On average, Amir responds to 4 leads each day. In this exercise, you’ll calculate probabilities of Amir responding to different numbers of leads.\n\n3.6.1 Instructions 100 XP\n\nWhat’s the probability that Amir responds to 5 leads in a day, given that he responds to an average of 4?\nAmir’s coworker responds to an average of 5.5 leads per day. What is the probability that she answers 5 leads in a day?\nWhat’s the probability that Amir responds to 2 or fewer leads in a day?\nWhat’s the probability that Amir responds to more than 10 leads in a day?\n\n\n\nex_21.R\n\ndpois(5, lambda=4)\ndpois(5, lambda = 5.5)\nppois(2, lambda = 4)\n1 - ppois(10, 4)"
  },
  {
    "objectID": "_introduction_to_statistics_in_r_03.html#modeling-time-between-leads",
    "href": "_introduction_to_statistics_in_r_03.html#modeling-time-between-leads",
    "title": "3  More Distributions and the Central Limit Theorem",
    "section": "3.7 Modeling time between leads",
    "text": "3.7 Modeling time between leads\nTo further evaluate Amir’s performance, you want to know how much time it takes him to respond to a lead after he opens it. On average, it takes 2.5 hours for him to respond. In this exercise, you’ll calculate probabilities of different amounts of time passing between Amir receiving a lead and sending a response.\n\n3.7.1 Instructions 100 XP\n\nWhat’s the probability it takes Amir less than an hour to respond to a lead?\nWhat’s the probability it takes Amir more than 4 hours to respond to a lead?\nWhat’s the probability it takes Amir 3-4 hours to respond to a lead?\n\n\n\nex_022.R\n\npexp(1, rate=1 / 2.5 )\n1 - pexp(4, rate = 1/2.5, lower.tail=FALSE)\npexp(4, rate = 1 / 2.5) - pexp(3, rate = 1 / 2.5 )"
  },
  {
    "objectID": "_introduction_to_regression_in_r_01.html",
    "href": "_introduction_to_regression_in_r_01.html",
    "title": "5  Simple Linear Regression",
    "section": "",
    "text": "You’ll learn the basics of this popular statistical model, what regression is, and how linear and logistic regressions differ. You’ll then learn how to fit simple linear regression models with numeric and categorical explanatory variables, and how to describe the relationship between the response and explanatory variables using model coefficients."
  },
  {
    "objectID": "_introduction_to_regression_in_r_00.html",
    "href": "_introduction_to_regression_in_r_00.html",
    "title": "Introduction to Regression in R: part description",
    "section": "",
    "text": "Linear regression and logistic regression are thetwo most widely used statistical models and act > like master keys, unlocking the secrets hidden in datasets. In this course, you’ll gain the skills you need to fit simple linear and logistic regressions. Through hands-on exercises, you’ll explore the relationships between variables in real- world datasets, including motor insurance claims, Taiwan house prices, fish sizes, and more. By the end of this course, you’ll know how to make predictions from your data, quantify model performance, and diagnose problems with model fit."
  },
  {
    "objectID": "_introduction_to_statistics_in_r_00.html",
    "href": "_introduction_to_statistics_in_r_00.html",
    "title": "Introduction to Statistics in R: part description",
    "section": "",
    "text": "Statistics is the study of how to collect, analyze, and draw conclusions from data. It’s a hugely valuable tool that you can use to bring the future into focus and infer the answer to tons of questions. For example, what is the likelihood of someone purchasing your product, how many calls will your support team receive, and how many jeans sizes should you manufacture to fit 95% of the population? In this course, you’ll use sales data to discover how to answer questions like these as you grow your statistical skills and learn how to calculate averages, use scatterplots to show the relationship between numeric values, and calculate correlation. You’ll also tackle probability, the backbone of statistical reasoning, and learn how to conduct a well-designed study to draw your own conclusions from data.\n\n\n\ncolab.r\n\n # run this in colab to start a notebook with R\n https://colab.research.google.com/#create=true&language=r"
  },
  {
    "objectID": "_introduction_to_regression_in_r_02.html",
    "href": "_introduction_to_regression_in_r_02.html",
    "title": "6  Predictions and model objects",
    "section": "",
    "text": "In this chapter, you’ll discover how to use linear regression models to make predictions on Taiwanese house prices and Facebook advert clicks. You’ll also grow your regression skills as you get hands-on with model objects, understand the concept of “regression to the mean”, and learn how to transform variables in a dataset."
  },
  {
    "objectID": "_introduction_to_regression_in_r_03.html",
    "href": "_introduction_to_regression_in_r_03.html",
    "title": "7  Assessing model fit",
    "section": "",
    "text": "In this chapter, you’ll learn how to ask questions of your model to assess fit. You’ll learn how to quantify how well a linear regression model fits, diagnose model problems using visualizations, and understand the leverage and influence of each observation used to create the model."
  },
  {
    "objectID": "_introduction_to_regression_in_r_04.html",
    "href": "_introduction_to_regression_in_r_04.html",
    "title": "8  Simple logistic regression",
    "section": "",
    "text": "Learn to fit logistic regression models. Using real- world data, you’ll predict the likelihood of a customer closing their bank account as probabilities of success and odds ratios, and quantify model performance using confusion matrices."
  }
]